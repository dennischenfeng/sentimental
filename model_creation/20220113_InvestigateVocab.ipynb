{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8c9c2e-1812-4bbb-b2cc-21454580cbf8",
   "metadata": {},
   "source": [
    "Investigate differences between using glove vocab vs using training-generated vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd9d693-e736-4518-be64-c7af9a39034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torchtext.vocab import vocab, Vocab, GloVe, build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "from typing import Callable, List, Tuple, Iterable\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_contour\n",
    "from optuna.importance import get_param_importances\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a316cfb5-f234-4206-949e-df06928625cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "SPECIAL_TOKENS = (PAD_TOKEN, EOS_TOKEN, UNK_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "639368d0-5071-4007-8d51-4760f5ed410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# glove embeddings --> vocab\n",
    "embedding_dim = 100\n",
    "embedding_vecs = GloVe(name='6B', dim=embedding_dim)\n",
    "\n",
    "embedding_dict = OrderedDict()\n",
    "embedding_dict.update({PAD_TOKEN: 1})\n",
    "embedding_dict.update({EOS_TOKEN: 1})\n",
    "embedding_dict.update({UNK_TOKEN: 1})\n",
    "embedding_dict.update(embedding_vecs.stoi)\n",
    "# min_freq=0 is a hack to read in the 0th token from embedding_vecs.stoi\n",
    "voc_glove = vocab(embedding_dict, min_freq=0)\n",
    "voc_glove.set_default_index(voc_glove[UNK_TOKEN])\n",
    "\n",
    "embedding = Embedding.from_pretrained(\n",
    "    embedding_vecs.vectors, freeze=True, padding_idx=voc_glove[PAD_TOKEN]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2c19da-6f11-4ab6-87cb-14377098b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_texts(\n",
    "    texts: Iterable[str], tokenizer: Callable, specials=SPECIAL_TOKENS, **kwargs\n",
    ") -> Vocab:\n",
    "    tk_seqs = [tokenizer(s) for s in tqdm(texts)]\n",
    "    voc = build_vocab_from_iterator(tk_seqs, specials=specials, **kwargs)\n",
    "    voc.set_default_index(voc[UNK_TOKEN])\n",
    "    return voc\n",
    "\n",
    "def nums_from_fractions(total: int, fractions: Tuple[float]) -> Tuple[int]:\n",
    "    \"\"\"\n",
    "    :param fractions: fractions of the total number. One elem must be -1, \n",
    "        which denotes \"remaining\"\n",
    "    \"\"\"\n",
    "    assert fractions.count(-1) == 1, (\n",
    "        \"Must have exactly one occurence of -1 to denote a fraction of 'remaining' items\"\n",
    "    )\n",
    "    nums = [int(total * f) if f != -1 else 0 for f in fractions]\n",
    "    idx_remaining = fractions.index(-1)\n",
    "    nums[idx_remaining] = total - sum(nums)\n",
    "    assert all([elem >= 0 for elem in nums])\n",
    "    return tuple(nums)\n",
    "\n",
    "assert nums_from_fractions(100, [0.7, 0.3, -1]) == (70, 30, 0)\n",
    "assert nums_from_fractions(100, [0.7, 0.155, -1]) == (70, 15, 15)\n",
    "assert nums_from_fractions(100, [0.7, 0, -1]) == (70, 0, 30)\n",
    "# tested that these lines raise error, as expected: \n",
    "# nums_from_fractions(100, [0.7, 0.3, -2])\n",
    "# nums_from_fractions(100, [0.7, 0.5, -1])\n",
    "\n",
    "def seqs_from_texts(texts: List[str], tokenizer: Callable, voc: Vocab) -> th.Tensor:\n",
    "    \"\"\"\n",
    "    Returns padded sequences (numericalized texts)\n",
    "    \"\"\"\n",
    "    nz_texts = [th.tensor(voc(tokenizer(text))) for text in texts]\n",
    "    seqs = pad_sequence(nz_texts, padding_value=voc[PAD_TOKEN])\n",
    "    return seqs\n",
    "\n",
    "def count_oov_rate(seqs: Iterable[th.Tensor], voc: Vocab) -> float:\n",
    "    num_oov = 0\n",
    "    num_tokens = 0\n",
    "    for i, item in enumerate(seqs):\n",
    "        # item = d[0][0]\n",
    "        num_oov += th.sum(item == voc[UNK_TOKEN]).item()\n",
    "        num_tokens += th.sum(item != voc[PAD_TOKEN]).item()\n",
    "    return num_oov / num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1049a-c1a8-44cf-bc62-390e1d0cdf46",
   "metadata": {},
   "source": [
    "# Disaster tweets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "174cd84f-34f0-410c-a542-b5158356a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_disaster_tweets.csv\")\n",
    "texts_train, texts_val, texts_test = random_split(\n",
    "    df.text, nums_from_fractions(len(df.text), [0.7, 0.15, -1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33587b65-c4c6-49c3-8b3e-f36290128a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5329, 1141, 1143)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_train), len(texts_val), len(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be8fef13-ed26-453d-afbf-2bb6a871c627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 5329/5329 [00:00<00:00, 12476.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15313158398774182"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using training-generated vocab\n",
    "voc_train = build_vocab_from_texts(texts_train, tokenizer)\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_train)\n",
    "count_oov_rate(seqs, voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "944a70db-15c4-4f4d-b6eb-f191a335a48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20720"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc_train.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9813e19-1231-4885-b293-9b90417383c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32503351848304923"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using glove vocab\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_glove)\n",
    "count_oov_rate(seqs, voc_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1a36b6f-1a81-472e-b6c6-93d5f4f12556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all \n",
      "\n",
      "Forest fire near La Ronge Sask. Canada \n",
      "\n",
      "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected \n",
      "\n",
      "13,000 people receive #wildfires evacuation orders in California  \n",
      "\n",
      "Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(df.text[i][:500], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62a8f3-3905-4052-ad00-d3d4b164c9df",
   "metadata": {},
   "source": [
    "# Sentiment140 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "421983bd-1a21-4305-9b55-3f2c785d2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_twitter_sentiment.csv\", header=None, encoding='latin-1')\n",
    "df = df.rename(columns={\n",
    "    0: \"sentiment_raw\",\n",
    "    5: \"text\",\n",
    "})\n",
    "texts_train, texts_val, texts_test = random_split(\n",
    "    df.text, nums_from_fractions(len(df.text), [0.7, 0.15, -1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c08e6c47-60d7-4df1-9806-2168f0f4124d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1120000, 240000, 240000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_train), len(texts_val), len(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ad044e0-368f-49a3-b83f-cc826e449542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1120000/1120000 [01:02<00:00, 17780.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.026524100834600644"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using training-generated vocab\n",
    "voc_train = build_vocab_from_texts(texts_train, tokenizer)\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_train)\n",
    "count_oov_rate(seqs, voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dcc6b34-1f1b-40fa-8356-d1810f491f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686201"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc_train.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60b5ac17-2b67-4c74-9095-afaed68938b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20403798900253428"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using glove vocab\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_glove)\n",
    "count_oov_rate(seqs, voc_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2af41020-f2e4-40d2-9179-a2de6dba2a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D \n",
      "\n",
      "is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah! \n",
      "\n",
      "@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds \n",
      "\n",
      "my whole body feels itchy and like its on fire  \n",
      "\n",
      "@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(df.text[i][:500], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b613e39-c15a-40e3-a483-7c9717c6122f",
   "metadata": {},
   "source": [
    "# Amazon reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d9cdce0-5a41-4c7e-8198-c5b0fea53b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53258it [00:01, 49708.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# used example code from \n",
    "# https://colab.research.google.com/drive/1Zv6MARGQcrBbLHyjPVVMZVnRWsRnVMpV#scrollTo=7igYuRaV4bF7\n",
    "\n",
    "data = []\n",
    "with gzip.open('data/data_reviews_Office_Products_5.json.gz') as f:\n",
    "    for l in tqdm(f):\n",
    "        data.append(json.loads(l.strip()))\n",
    "    \n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df = df.rename(columns={\n",
    "    \"reviewText\": \"text\",\n",
    "})\n",
    "\n",
    "texts_train, texts_val, texts_test = random_split(\n",
    "    df.text, nums_from_fractions(len(df.text), [0.7, 0.15, -1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab412658-0fad-4205-9c0a-1be838ae1877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37280, 7988, 7990)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_train), len(texts_val), len(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aee7a374-04a4-4441-ad74-eeeeb6b019a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 37280/37280 [00:14<00:00, 2528.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.007912226728692733"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using training-generated vocab\n",
    "voc_train = build_vocab_from_texts(texts_train, tokenizer)\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_train)\n",
    "count_oov_rate(seqs, voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dca2e184-80bd-4497-a63c-6732065604f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76931"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc_train.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "321f84f4-0898-466a-9395-bd1a28e69d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11531525777366163"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using glove vocab\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_glove)\n",
    "count_oov_rate(seqs, voc_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4161d41-bd4b-49c3-ba4f-7c1838c6126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I bought my first HP12C in about 1984 or so, and it served me faithfully until 2002 when I lost it while travelling.  I searched for another one to replace it, but found one difficult to come by in my area.  So, I decided to buy up and purchased an HP 49G.  What a mistake!  I know that many people view the HP 49G (now 49G+) as the flagship of the HP line, but for me that was a disaster.The 49G may be powerful, but using it is extremely counterintuitive...and the manual was sketchy at best.  The  \n",
      "\n",
      "WHY THIS BELATED REVIEW? I feel very obliged to share my views about this old workhorse, the HP12C GOLD at its 25th anniversary and my 1Oth year as a satisfied ower user and fan. Especially after sharing my frank views on Amazon about two of its three successors on Amazon.com, burying the HP12c Platinum (2004 1st release, HP12cP) and praising the HP12c Platiunum Anniversary Edition (2006 3rd release, HP12cPAE or HP12cPA).To the majority of HP 12c calculator owners and fans, including the non-tec \n",
      "\n",
      "I have an HP 48GX that has been kicking for more than twenty years and an HP 11 that is more than 25 years old and still flawless.  I have had this 12C for about 18 months and the number 5 key is already giving me trouble (have to press hard for it to register).  No drops, no abuse.  The overall feel of this new HP is cheaper plastic.  The keys feel hollow and light.  The available functions are great.  This is a classic calculator.  But the functions are no good if the keys stop working... \n",
      "\n",
      "I've started doing more finance stuff recently and went looking for a good time-value-of-money calculator. I was pleasantly surprised to find the HP12C was still available. I've been using HP calculators for decades (HP-45, HP-15 and now an HP-32SII) because of their great feel and long-lasting quality. However, it's also true that the HP12C is probably the last relic of the great HP calculator legacy. It has 'Made In China' stamped on the rear, but the keyboard feel seems much like my dear depa \n",
      "\n",
      "For simple calculations and discounted cash flows, this one is still the best.  I used this in my graduate business program, and for years as a practicing CPA and financial executive.  Of course for complex cash flows you will want to use an Excel spreadsheet; but for quick cash flow calculations the HP12C really is the tool you want.Personally I find Reverse Polish Notation (&#34;RPN&#34;), which is what this calculator uses, to be easy and intuitive.  Many do; some don't.  RPN takes the place  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(df.text[i][:500], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f15aff-3752-4bac-8537-23d9c30d38ce",
   "metadata": {},
   "source": [
    "# Movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00f1968a-3c95-4320-b705-09c81cd7f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 12500/12500 [00:28<00:00, 440.05it/s]\n",
      "100%|████████████████████████████████████████████████████████| 12500/12500 [00:35<00:00, 356.94it/s]\n",
      "100%|████████████████████████████████████████████████████████| 12500/12500 [00:40<00:00, 307.04it/s]\n",
      "100%|████████████████████████████████████████████████████████| 12500/12500 [00:45<00:00, 275.46it/s]\n"
     ]
    }
   ],
   "source": [
    "basepath = \"data/stanford_movie_reviews/aclImdb/\"\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in tqdm(sorted(os.listdir(path))):\n",
    "            with open(os.path.join(path, file),\n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]],\n",
    "                           ignore_index=True)\n",
    "df.columns = ['review', 'sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6d88c279-4c1e-4db4-a1e9-c25deb2e67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    \"review\": \"text\",\n",
    "})\n",
    "\n",
    "texts_train, texts_val, texts_test = random_split(\n",
    "    df.text, nums_from_fractions(len(df.text), [0.7, 0.15, -1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ea597964-57b4-43ae-bf4e-3531ee201058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35000, 7500, 7500)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_train), len(texts_val), len(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "610e1d03-6673-417f-ba46-dc922f09523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 35000/35000 [00:24<00:00, 1404.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008642635768701648"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using training-generated vocab\n",
    "voc_train = build_vocab_from_texts(texts_train, tokenizer)\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_train)\n",
    "count_oov_rate(seqs, voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79bb916b-6d4f-4989-bd9a-120a4734bd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146580"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc_train.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b3768044-b46f-48d1-aba9-7a58914b21da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13025608124778243"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using glove vocab\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_glove)\n",
    "count_oov_rate(seqs, voc_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c6aa969-f1a8-43ec-9ed9-a33cfca25913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400003"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc_glove.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95255859-9026-4ab0-9d77-7e7a5332c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the  \n",
      "\n",
      "Actor turned director Bill Paxton follows up his promising debut, the Gothic-horror \"Frailty\", with this family friendly sports drama about the 1913 U.S. Open where a young American caddy rises from his humble background to play against his Bristish idol in what was dubbed as \"The Greatest Game Ever Played.\" I'm no fan of golf, and these scrappy underdog sports flicks are a dime a dozen (most recently done to grand effect with \"Miracle\" and \"Cinderella Man\"), but some how this film was enthralli \n",
      "\n",
      "As a recreational golfer with some knowledge of the sport's history, I was pleased with Disney's sensitivity to the issues of class in golf in the early twentieth century. The movie depicted well the psychological battles that Harry Vardon fought within himself, from his childhood trauma of being evicted to his own inability to break that glass ceiling that prevents him from being accepted as an equal in English golf society. Likewise, the young Ouimet goes through his own class struggles, being \n",
      "\n",
      "I saw this film in a sneak preview, and it is delightful. The cinematography is unusually creative, the acting is good, and the story is fabulous. If this movie does not do well, it won't be because it doesn't deserve to. Before this film, I didn't realize how charming Shia Lebouf could be. He does a marvelous, self-contained, job as the lead. There's something incredibly sweet about him, and it makes the movie even better. The other actors do a good job as well, and the film contains moments of \n",
      "\n",
      "Bill Paxton has taken the true story of the 1913 US golf open and made a film that is about much more than an extra-ordinary game of golf. The film also deals directly with the class tensions of the early twentieth century and touches upon the profound anti-Catholic prejudices of both the British and American establishments. But at heart the film is about that perennial favourite of triumph against the odds.<br /><br />The acting is exemplary throughout. Stephen Dillane is excellent as usual, bu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(df.text[i][:500], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a0bca-b3cf-4731-838e-256a7ea4c101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
