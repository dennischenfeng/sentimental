{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8c9c2e-1812-4bbb-b2cc-21454580cbf8",
   "metadata": {},
   "source": [
    "Investigate differences between using glove vocab vs using training-generated vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ccd9d693-e736-4518-be64-c7af9a39034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torchtext.vocab import vocab, Vocab, GloVe, build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "from typing import Callable, List, Tuple, Iterable\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_contour\n",
    "from optuna.importance import get_param_importances\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a316cfb5-f234-4206-949e-df06928625cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "SPECIAL_TOKENS = (PAD_TOKEN, EOS_TOKEN, UNK_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "639368d0-5071-4007-8d51-4760f5ed410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# glove embeddings --> vocab\n",
    "embedding_dim = 100\n",
    "embedding_vecs = GloVe(name='6B', dim=embedding_dim)\n",
    "\n",
    "embedding_dict = OrderedDict()\n",
    "embedding_dict.update({PAD_TOKEN: 1})\n",
    "embedding_dict.update({EOS_TOKEN: 1})\n",
    "embedding_dict.update({UNK_TOKEN: 1})\n",
    "embedding_dict.update(embedding_vecs.stoi)\n",
    "# min_freq=0 is a hack to read in the 0th token from embedding_vecs.stoi\n",
    "voc_glove = vocab(embedding_dict, min_freq=0)\n",
    "voc_glove.set_default_index(voc_glove[UNK_TOKEN])\n",
    "\n",
    "embedding = Embedding.from_pretrained(\n",
    "    embedding_vecs.vectors, freeze=True, padding_idx=voc_glove[PAD_TOKEN]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d2c19da-6f11-4ab6-87cb-14377098b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_texts(\n",
    "    texts: Iterable[str], tokenizer: Callable, specials=SPECIAL_TOKENS, **kwargs\n",
    ") -> Vocab:\n",
    "    tk_seqs = [tokenizer(s) for s in tqdm(texts)]\n",
    "    voc = build_vocab_from_iterator(tk_seqs, specials=specials, **kwargs)\n",
    "    voc.set_default_index(voc[UNK_TOKEN])\n",
    "    return voc\n",
    "\n",
    "def nums_from_fractions(total: int, fractions: Tuple[float]) -> Tuple[int]:\n",
    "    \"\"\"\n",
    "    :param fractions: fractions of the total number. One elem must be -1, \n",
    "        which denotes \"remaining\"\n",
    "    \"\"\"\n",
    "    assert fractions.count(-1) == 1, (\n",
    "        \"Must have exactly one occurence of -1 to denote a fraction of 'remaining' items\"\n",
    "    )\n",
    "    nums = [int(total * f) if f != -1 else 0 for f in fractions]\n",
    "    idx_remaining = fractions.index(-1)\n",
    "    nums[idx_remaining] = total - sum(nums)\n",
    "    assert all([elem >= 0 for elem in nums])\n",
    "    return tuple(nums)\n",
    "\n",
    "assert nums_from_fractions(100, [0.7, 0.3, -1]) == (70, 30, 0)\n",
    "assert nums_from_fractions(100, [0.7, 0.155, -1]) == (70, 15, 15)\n",
    "assert nums_from_fractions(100, [0.7, 0, -1]) == (70, 0, 30)\n",
    "# tested that these lines raise error, as expected: \n",
    "# nums_from_fractions(100, [0.7, 0.3, -2])\n",
    "# nums_from_fractions(100, [0.7, 0.5, -1])\n",
    "\n",
    "def seqs_from_texts(texts: List[str], tokenizer: Callable, voc: Vocab) -> th.Tensor:\n",
    "    \"\"\"\n",
    "    Returns padded sequences (numericalized texts)\n",
    "    \"\"\"\n",
    "    nz_texts = [th.tensor(voc(tokenizer(text))) for text in texts]\n",
    "    seqs = pad_sequence(nz_texts, padding_value=voc[PAD_TOKEN])\n",
    "    return seqs\n",
    "\n",
    "def count_oov_rate(seqs: Iterable[th.Tensor], voc: Vocab) -> float:\n",
    "    num_oov = 0\n",
    "    num_tokens = 0\n",
    "    for i, item in enumerate(seqs):\n",
    "        # item = d[0][0]\n",
    "        num_oov += th.sum(item == voc[UNK_TOKEN]).item()\n",
    "        num_tokens += th.sum(item != voc[PAD_TOKEN]).item()\n",
    "    return num_oov / num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1049a-c1a8-44cf-bc62-390e1d0cdf46",
   "metadata": {},
   "source": [
    "# Disaster tweets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "174cd84f-34f0-410c-a542-b5158356a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_disaster_tweets.csv\")\n",
    "texts_train, texts_val, texts_test = random_split(\n",
    "    df.text, nums_from_fractions(len(df.text), [0.7, 0.15, -1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33587b65-c4c6-49c3-8b3e-f36290128a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5329, 1141, 1143)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_train), len(texts_val), len(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be8fef13-ed26-453d-afbf-2bb6a871c627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 5329/5329 [00:00<00:00, 16799.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1528582850289826"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using training-generated vocab\n",
    "voc_train = build_vocab_from_texts(texts_train, tokenizer)\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_train)\n",
    "count_oov_rate(seqs, voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9813e19-1231-4885-b293-9b90417383c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3126124325404757"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using glove vocab\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_glove)\n",
    "count_oov_rate(seqs, voc_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a36b6f-1a81-472e-b6c6-93d5f4f12556",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(df.text[i][:500], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62a8f3-3905-4052-ad00-d3d4b164c9df",
   "metadata": {},
   "source": [
    "# Sentiment140 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "421983bd-1a21-4305-9b55-3f2c785d2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_twitter_sentiment.csv\", header=None, encoding='latin-1')\n",
    "df = df.rename(columns={\n",
    "    0: \"sentiment_raw\",\n",
    "    5: \"text\",\n",
    "})\n",
    "texts_train, texts_val, texts_test = random_split(\n",
    "    df.text, nums_from_fractions(len(df.text), [0.7, 0.15, -1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c08e6c47-60d7-4df1-9806-2168f0f4124d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1120000, 240000, 240000)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_train), len(texts_val), len(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7ad044e0-368f-49a3-b83f-cc826e449542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1120000/1120000 [01:04<00:00, 17379.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02634319577525798"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using training-generated vocab\n",
    "voc_train = build_vocab_from_texts(texts_train, tokenizer)\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_train)\n",
    "count_oov_rate(seqs, voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "60b5ac17-2b67-4c74-9095-afaed68938b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2038886550684923"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using glove vocab\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_glove)\n",
    "count_oov_rate(seqs, voc_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2af41020-f2e4-40d2-9179-a2de6dba2a39",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m[i][:\u001b[38;5;241m500\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/sentimental/.venv/lib/python3.9/site-packages/pandas/core/generic.py:5487\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5481\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5482\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5483\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5484\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5485\u001b[0m ):\n\u001b[1;32m   5486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(df.text[i][:500], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b613e39-c15a-40e3-a483-7c9717c6122f",
   "metadata": {},
   "source": [
    "# Amazon reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1d9cdce0-5a41-4c7e-8198-c5b0fea53b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53258it [00:01, 45504.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# used example code from \n",
    "# https://colab.research.google.com/drive/1Zv6MARGQcrBbLHyjPVVMZVnRWsRnVMpV#scrollTo=7igYuRaV4bF7\n",
    "\n",
    "data = []\n",
    "with gzip.open('data/data_reviews_Office_Products_5.json.gz') as f:\n",
    "    for l in tqdm(f):\n",
    "        data.append(json.loads(l.strip()))\n",
    "    \n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df = df.rename(columns={\n",
    "    \"reviewText\": \"text\",\n",
    "})\n",
    "\n",
    "texts_train, texts_val, texts_test = random_split(\n",
    "    df.text, nums_from_fractions(len(df.text), [0.7, 0.15, -1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab412658-0fad-4205-9c0a-1be838ae1877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37280, 7988, 7990)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_train), len(texts_val), len(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aee7a374-04a4-4441-ad74-eeeeb6b019a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 37280/37280 [00:12<00:00, 2893.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.007513979454133859"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using training-generated vocab\n",
    "voc_train = build_vocab_from_texts(texts_train, tokenizer)\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_train)\n",
    "count_oov_rate(seqs, voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "321f84f4-0898-466a-9395-bd1a28e69d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11351353328188943"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using glove vocab\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_glove)\n",
    "count_oov_rate(seqs, voc_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4161d41-bd4b-49c3-ba4f-7c1838c6126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(df.text[i][:500], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f15aff-3752-4bac-8537-23d9c30d38ce",
   "metadata": {},
   "source": [
    "# Movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00f1968a-3c95-4320-b705-09c81cd7f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 12500/12500 [00:28<00:00, 440.05it/s]\n",
      "100%|████████████████████████████████████████████████████████| 12500/12500 [00:35<00:00, 356.94it/s]\n",
      "100%|████████████████████████████████████████████████████████| 12500/12500 [00:40<00:00, 307.04it/s]\n",
      "100%|████████████████████████████████████████████████████████| 12500/12500 [00:45<00:00, 275.46it/s]\n"
     ]
    }
   ],
   "source": [
    "basepath = \"data/stanford_movie_reviews/aclImdb/\"\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in tqdm(sorted(os.listdir(path))):\n",
    "            with open(os.path.join(path, file),\n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]],\n",
    "                           ignore_index=True)\n",
    "df.columns = ['review', 'sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6d88c279-4c1e-4db4-a1e9-c25deb2e67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    \"review\": \"text\",\n",
    "})\n",
    "\n",
    "texts_train, texts_val, texts_test = random_split(\n",
    "    df.text, nums_from_fractions(len(df.text), [0.7, 0.15, -1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ea597964-57b4-43ae-bf4e-3531ee201058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35000, 7500, 7500)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_train), len(texts_val), len(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "610e1d03-6673-417f-ba46-dc922f09523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 35000/35000 [00:24<00:00, 1404.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008642635768701648"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using training-generated vocab\n",
    "voc_train = build_vocab_from_texts(texts_train, tokenizer)\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_train)\n",
    "count_oov_rate(seqs, voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79bb916b-6d4f-4989-bd9a-120a4734bd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146580"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc_train.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b3768044-b46f-48d1-aba9-7a58914b21da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13025608124778243"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov rate, using glove vocab\n",
    "seqs = seqs_from_texts(texts_test, tokenizer, voc_glove)\n",
    "count_oov_rate(seqs, voc_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c6aa969-f1a8-43ec-9ed9-a33cfca25913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400003"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc_glove.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95255859-9026-4ab0-9d77-7e7a5332c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the  \n",
      "\n",
      "Actor turned director Bill Paxton follows up his promising debut, the Gothic-horror \"Frailty\", with this family friendly sports drama about the 1913 U.S. Open where a young American caddy rises from his humble background to play against his Bristish idol in what was dubbed as \"The Greatest Game Ever Played.\" I'm no fan of golf, and these scrappy underdog sports flicks are a dime a dozen (most recently done to grand effect with \"Miracle\" and \"Cinderella Man\"), but some how this film was enthralli \n",
      "\n",
      "As a recreational golfer with some knowledge of the sport's history, I was pleased with Disney's sensitivity to the issues of class in golf in the early twentieth century. The movie depicted well the psychological battles that Harry Vardon fought within himself, from his childhood trauma of being evicted to his own inability to break that glass ceiling that prevents him from being accepted as an equal in English golf society. Likewise, the young Ouimet goes through his own class struggles, being \n",
      "\n",
      "I saw this film in a sneak preview, and it is delightful. The cinematography is unusually creative, the acting is good, and the story is fabulous. If this movie does not do well, it won't be because it doesn't deserve to. Before this film, I didn't realize how charming Shia Lebouf could be. He does a marvelous, self-contained, job as the lead. There's something incredibly sweet about him, and it makes the movie even better. The other actors do a good job as well, and the film contains moments of \n",
      "\n",
      "Bill Paxton has taken the true story of the 1913 US golf open and made a film that is about much more than an extra-ordinary game of golf. The film also deals directly with the class tensions of the early twentieth century and touches upon the profound anti-Catholic prejudices of both the British and American establishments. But at heart the film is about that perennial favourite of triumph against the odds.<br /><br />The acting is exemplary throughout. Stephen Dillane is excellent as usual, bu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(df.text[i][:500], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a0bca-b3cf-4731-838e-256a7ea4c101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
