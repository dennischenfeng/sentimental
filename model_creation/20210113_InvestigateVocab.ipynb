{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8c9c2e-1812-4bbb-b2cc-21454580cbf8",
   "metadata": {},
   "source": [
    "Investigate differences between using glove vocab vs using training-generated vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccd9d693-e736-4518-be64-c7af9a39034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torchtext.vocab import vocab, Vocab, GloVe, build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "from typing import Callable, List, Tuple, Iterable\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_contour\n",
    "from optuna.importance import get_param_importances\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a316cfb5-f234-4206-949e-df06928625cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "SPECIAL_TOKENS = (PAD_TOKEN, EOS_TOKEN, UNK_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639368d0-5071-4007-8d51-4760f5ed410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# glove embeddings --> vocab\n",
    "# embedding_dim = 100\n",
    "# embedding_vecs = GloVe(name='6B', dim=embedding_dim)\n",
    "\n",
    "# embedding_dict = OrderedDict()\n",
    "# embedding_dict.update({PAD_TOKEN: 1})\n",
    "# embedding_dict.update({EOS_TOKEN: 1})\n",
    "# embedding_dict.update({UNK_TOKEN: 1})\n",
    "# embedding_dict.update(embedding_vecs.stoi)\n",
    "# # min_freq=0 is a hack to read in the 0th token from embedding_vecs.stoi\n",
    "# voc = vocab(embedding_dict, min_freq=0)\n",
    "# voc.set_default_index(voc[UNK_TOKEN])\n",
    "\n",
    "# # glove embeddings --> embedding module\n",
    "# embedding = Embedding.from_pretrained(\n",
    "#     embedding_vecs.vectors, freeze=True, padding_idx=voc[PAD_TOKEN]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d2c19da-6f11-4ab6-87cb-14377098b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_texts(texts: Iterable[str], tokenizer: Callable, **kwargs) -> Vocab:\n",
    "    tk_seqs = [tokenizer(s) for s in texts]\n",
    "    voc = build_vocab_from_iterator(tk_seqs, **kwargs)\n",
    "    voc.set_default_index(voc[UNK_TOKEN])\n",
    "    return voc\n",
    "\n",
    "def num_train_val_test_from_ratios(\n",
    "    n: int, train_ratio: float = 0.7, val_ratio: float = 0.15\n",
    ") -> Tuple[int]:\n",
    "    num_train = int(train_ratio * n)\n",
    "    num_val = int(val_ratio * n)\n",
    "    num_test = n - num_train - num_val\n",
    "    assert num_train > 0\n",
    "    assert num_val > 0\n",
    "    assert num_test > 0, num_test\n",
    "    \n",
    "    return num_train, num_val, num_test\n",
    "\n",
    "assert num_train_val_test_from_ratios(10, 0.7, 0.15) == (7, 1, 2)\n",
    "\n",
    "def oov_rate(seqs: Iterable[th.Tensor], voc: Vocab) -> float:\n",
    "    num_oov = 0\n",
    "    num_tokens = 0\n",
    "    for i, item in enumerate(seqs):\n",
    "        # item = d[0][0]\n",
    "        num_oov += th.sum(item == voc[UNK_TOKEN]).item()\n",
    "        num_tokens += th.sum(item != voc[PAD_TOKEN]).item()\n",
    "    return num_oov / num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "174cd84f-34f0-410c-a542-b5158356a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_disaster_tweets.csv\")\n",
    "# voc = build_vocab_from_texts(df.text, tokenizer, specials=SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2bed507-e175-4582-941b-b86a587d62ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsplit\\nbuild voc from train texts\\noov_rate\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "split\n",
    "build voc from train texts\n",
    "oov_rate\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "114ec90d-4d37-40c8-99f8-f7daa95b4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = random_split(\n",
    "    df.text, num_train_val_test_from_ratios(len(df.text))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b0701f6-976e-4bac-8f08-dfb33a8fee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = build_vocab_from_texts(train_set, tokenizer, specials=SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "86ce425d-f414-49a2-8109-a47cf472b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_texts = [th.tensor(voc(tokenizer(text))) for text in test_set]\n",
    "seqs = pad_sequence(nz_texts, padding_value=voc[PAD_TOKEN])\n",
    "# oov_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3e802be-e94c-4f10-986d-3bba60b32353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1549107804853937"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_rate(seqs, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66fd6d-9af1-4aa5-8ccc-d5e1f736a00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388dbac2-839b-49b0-8e10-9b6f6c51f3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56bef4b7-618d-41a0-94e6-d8ef88c2f722",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310d0964-d0ee-432b-91ed-041864d756c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDisasterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Disaster tweet data. Download: dataset from \n",
    "    https://www.kaggle.com/c/nlp-getting-started, rename the `train.csv` \n",
    "    as `data_disaster_tweets.csv`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, tokenizer: Callable, voc: Vocab, df: Optional[pd.DataFrame] = None\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.voc = voc\n",
    "        \n",
    "        if not df:\n",
    "            # Load data and remove unnecessary columns\n",
    "            df = pd.read_csv(\"data/data_disaster_tweets.csv\")\n",
    "            df = df[[\"text\", \"target\"]]\n",
    "            df = df.reset_index(drop=True)\n",
    "        \n",
    "        # TODO: test\n",
    "        # print(\"Warning: testing only with 1000 data points\")\n",
    "        # df = df.iloc[0:1000, :] \n",
    "        \n",
    "        nz_texts = []  # numericalized_texts\n",
    "        seq_lengths = []  # sequence lengths\n",
    "        for text in tqdm(df.text):\n",
    "            nz_text = th.tensor(self.voc(self.tokenizer(text)))\n",
    "            nz_texts.append(nz_text)\n",
    "            seq_lengths.append(len(nz_text))\n",
    "        \n",
    "        # shape of x is: T x B, where T is length of longest seq, B is batch size\n",
    "        self.seqs = pad_sequence(nz_texts, padding_value=self.voc[PAD_TOKEN])\n",
    "        self.seq_lengths = th.tensor(seq_lengths)\n",
    "        self.targets = th.tensor(df.target).float()\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, i: int) -> Tuple[Tuple[th.Tensor, int], float]:\n",
    "        seq = self.seqs[:, i]\n",
    "        seq_length = self.seq_lengths[i]\n",
    "        targets = self.targets[i]\n",
    "        return (seq, seq_length), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e37dae5-f327-44c8-8a26-acab88372bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 7613/7613 [00:01<00:00, 4015.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.93 s, sys: 122 ms, total: 2.05 s\n",
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_ds = TwitterDisasterDataset(tokenizer, voc)\n",
    "\n",
    "num_train = int(0.7 * len(full_ds))\n",
    "num_val = int(0.15 * len(full_ds))\n",
    "num_test = len(full_ds) - num_train - num_val\n",
    "batch_size = 64\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(full_ds, [num_train, num_val, num_test])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)  \n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "115ef623-82a4-4e2f-9a73-b4031ee06569",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(val_dl))\n",
    "assert len(test) == 2\n",
    "assert len(test[0]) == 2\n",
    "assert isinstance(test[0][0], th.Tensor)\n",
    "assert isinstance(test[0][1], th.Tensor)\n",
    "assert isinstance(test[1], th.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42f12a-bb7b-4e90-bd4e-d3a2842bf738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acec855-a0ad-4f78-8701-b0acfc71b0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b783a-1bc6-4bb3-abfd-978aa8a4efef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90581da9-9747-4fce-b20c-0df2047cc948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ad191-8e33-4c2a-b14e-be86f75223df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07228093-5b52-43ba-b0d6-dfccc5a7f778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ae20e-6d79-40d8-9e4a-b9ef030f064c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fe441d8-e727-4ff6-8e9f-9799f7f95221",
   "metadata": {},
   "source": [
    "# Investigate: oov rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4243c179-1fff-4c8e-b818-e5b5a53319ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov_rate(dl: DataLoader, voc: Vocab) -> float:\n",
    "    num_oov = 0\n",
    "    num_tokens = 0\n",
    "    for i, d in enumerate(dl):\n",
    "        seqs = d[0][0]\n",
    "        num_oov += th.sum(seqs == voc[UNK_TOKEN]).item()\n",
    "        num_tokens += th.sum(seqs != voc[PAD_TOKEN]).item()\n",
    "    return num_oov / num_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63b1b91e-a503-4ee5-9d99-3fe1f2446e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3223282906769"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_oov_rate(train_dl, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98f7058a-6b70-4fee-8805-a5071ec16d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33188698782111337"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_oov_rate(val_dl, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d26dadd7-7370-4bd1-8ac4-064992a210b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3217611780194061"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_oov_rate(test_dl, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43324825-5806-438a-9a49-bd0d9816445e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5329"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a701137c-d822-4f8f-a562-433dfed4f928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce1ab30e-9ff5-4a35-8967-73841352d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(train_dl))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a05ff2d-943b-4c11-9a28-394395129292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.sum(test == voc[UNK_TOKEN]).item()\n",
    "th.sum(test != voc[PAD_TOKEN]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9cdce0-5a41-4c7e-8198-c5b0fea53b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
