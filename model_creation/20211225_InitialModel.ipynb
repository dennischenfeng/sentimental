{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f85f4b-f1bf-441a-855c-45f3ebaa50a1",
   "metadata": {},
   "source": [
    "Here, I build a initial version of the sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4b7cd768-e810-421d-9407-a6511a327104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torchtext.vocab import vocab, Vocab, GloVe\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from typing import Callable, List, Tuple\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ad9a36-4d36-4513-9da0-53a297a9ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f31690-77d5-493e-aef8-b436d7d7fd6f",
   "metadata": {},
   "source": [
    "# 2. Use pre-trained glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddfd9a00-2836-4967-8dca-b75cdf26399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "SPECIAL_TOKENS = (PAD_TOKEN, EOS_TOKEN, UNK_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46cc992-3614-4206-8d81-301e2fdbedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_vecs = GloVe(name='6B', dim=embedding_dim, )\n",
    "\n",
    "# Use vocab from embedding_vecs\n",
    "embedding_dict = OrderedDict()\n",
    "embedding_dict.update({PAD_TOKEN: 1})\n",
    "embedding_dict.update({EOS_TOKEN: 1})\n",
    "embedding_dict.update({UNK_TOKEN: 1})\n",
    "embedding_dict.update(embedding_vecs.stoi)\n",
    "# min_freq=0 is a hack to read in the 0th token from embedding_vecs.stoi\n",
    "voc = vocab(embedding_dict, min_freq=0)\n",
    "voc.set_default_index(voc[UNK_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "587dbad7-8d6e-4682-b3a2-aae3e14f7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Sentiment data from tweets. Raw dataset downloaded from: \n",
    "    http://help.sentiment140.com/for-students\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: Callable, voc: Vocab) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.voc = voc\n",
    "        \n",
    "        # Load data and remove unnecessary columns\n",
    "        df = pd.read_csv(\"data_twitter_sentiment.csv\", header=None, encoding='latin-1')\n",
    "        df = df.rename(columns={\n",
    "            0: \"sentiment_raw\",\n",
    "            5: \"text\",\n",
    "        })\n",
    "        # raw sentiment data is from 0 to 4. Scale it to 0 and 1.\n",
    "        df[\"sentiment\"] = df.sentiment_raw / 4\n",
    "        df = df[[\"sentiment\", \"text\"]]\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        nz_texts = []  # numericalized_texts\n",
    "        seq_lengths = []  # sequence lengths\n",
    "        for text in tqdm(df.text):\n",
    "            nz_text = th.tensor(self.voc(self.tokenizer(text)))\n",
    "            nz_texts.append(nz_text)\n",
    "            seq_lengths.append(len(nz_text))\n",
    "        \n",
    "        # shape of x is: T x B, where T is length of longest seq, B is batch size\n",
    "        self.seqs = pad_sequence(nz_texts, padding_value=self.voc[PAD_TOKEN])\n",
    "        self.seq_lengths = seq_lengths\n",
    "        self.sentiments = df.sentiment\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sentiments)\n",
    "    \n",
    "    def __getitem__(self, i: int) -> Tuple[Tuple[th.Tensor, int], float]:\n",
    "        seq = self.seqs[:, i]\n",
    "        seq_length = self.seq_lengths[i]\n",
    "        sentiment = self.sentiments[i]\n",
    "        return (seq, seq_length), sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "217d821b-fb35-45fd-804c-9df548a0cd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1600000/1600000 [01:41<00:00, 15817.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 10.2 s, total: 1min 57s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_ds = TwitterDataset(tokenizer, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e0b1095-da25-49aa-99db-0d36e20ada0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([181, 1600000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ds.seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c321da5-e43d-4b61-ada4-58f624dee4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(0.7 * len(full_ds))\n",
    "num_val = int(0.15 * len(full_ds))\n",
    "num_test = len(full_ds) - num_train - num_val\n",
    "batch_size = 64\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(full_ds, [num_train, num_val, num_test])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "# no need to shuffle val or test dl's\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)  \n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6f66391f-5816-404b-88b8-cb98bd0cc83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cb6315cb-b47f-44be-be39-1e363896a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitVanillaRNN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, embedding: nn.Embedding, hidden_size: int = 128, num_layers: int = 1,\n",
    "        lr: float = 1e-3, dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # TODO: try using dropout in rnn\n",
    "        # TODO: try using bidirectional in rnn\n",
    "        self.rnn = nn.RNN(\n",
    "            self.embedding.embedding_dim, self.hidden_size, batch_first=True, \n",
    "            dropout=self.dropout, num_layers=self.num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.mse = MeanSquaredError()\n",
    "    \n",
    "    def forward(self, x: List[th.Tensor]) -> th.Tensor:\n",
    "        assert len(x) == 2\n",
    "        seqs, seq_lengths = x\n",
    "        embedded = self.embedding(seqs)\n",
    "        packed = pack_padded_sequence(\n",
    "            embedded, seq_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # TODO: try usng a randomly generated initialhidden state \n",
    "        # (instead of the zero vector default)\n",
    "        _, h_n = self.rnn(packed)\n",
    "        \n",
    "        assert h_n.shape == (1, batch_size, self.hidden_size)\n",
    "        \n",
    "        x = h_n[-1, :, :]\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"test\")\n",
    "    \n",
    "    def generalized_step(self, batch: th.Tensor, batch_idx: int, label: str) -> th.Tensor:\n",
    "        x, y = batch\n",
    "        predicted = self(x)\n",
    "        loss = F.mse_loss(predicted, y)\n",
    "        self.log(f\"{label}_loss\", loss)\n",
    "        self.log(f\"{label}_mse\", self.mse(predicted, y))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ffcb8d27-0e4e-464e-b6eb-9108bfff89c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding.from_pretrained(\n",
    "    embedding_vecs.vectors, freeze=True, padding_idx=voc[PAD_TOKEN]\n",
    ")\n",
    "\n",
    "vrnn = LitVanillaRNN(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e0516cf3-2864-49a3-a347-ed3b7cf02600",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n",
      "tensor([[0.4678],\n",
      "        [0.5019],\n",
      "        [0.5383],\n",
      "        [0.5361],\n",
      "        [0.5526],\n",
      "        [0.5562],\n",
      "        [0.4714],\n",
      "        [0.5238],\n",
      "        [0.5283],\n",
      "        [0.4851],\n",
      "        [0.4942],\n",
      "        [0.4423],\n",
      "        [0.5289],\n",
      "        [0.5247],\n",
      "        [0.5554],\n",
      "        [0.4696],\n",
      "        [0.5212],\n",
      "        [0.5124],\n",
      "        [0.5071],\n",
      "        [0.4654],\n",
      "        [0.5215],\n",
      "        [0.5088],\n",
      "        [0.5284],\n",
      "        [0.5375],\n",
      "        [0.4872],\n",
      "        [0.5141],\n",
      "        [0.4798],\n",
      "        [0.4934],\n",
      "        [0.5024],\n",
      "        [0.5225],\n",
      "        [0.4622],\n",
      "        [0.5261],\n",
      "        [0.5177],\n",
      "        [0.5048],\n",
      "        [0.5240],\n",
      "        [0.5191],\n",
      "        [0.5015],\n",
      "        [0.5147],\n",
      "        [0.4923],\n",
      "        [0.4576],\n",
      "        [0.6168],\n",
      "        [0.4885],\n",
      "        [0.5313],\n",
      "        [0.4751],\n",
      "        [0.4979],\n",
      "        [0.4955],\n",
      "        [0.5229],\n",
      "        [0.4992],\n",
      "        [0.5071],\n",
      "        [0.5034],\n",
      "        [0.5128],\n",
      "        [0.5214],\n",
      "        [0.5148],\n",
      "        [0.4768],\n",
      "        [0.4912],\n",
      "        [0.5127],\n",
      "        [0.5121],\n",
      "        [0.5087],\n",
      "        [0.5359],\n",
      "        [0.5137],\n",
      "        [0.5134],\n",
      "        [0.5052],\n",
      "        [0.5185],\n",
      "        [0.5247]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test = next(iter(val_dl))\n",
    "print(vrnn(test[0]).shape)\n",
    "print(vrnn(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18728346-2be7-4a69-a86a-45e0b9503404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e54f7-acb5-4856-b8fb-e186e01b7262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c28f3b-84d1-4a67-8641-e698b8a6875d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ea700-6f7a-45ff-ab95-2bf3ebc02cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0cd3cf-7bbe-4aa2-8f53-533cc4d81217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc30bfd-6bbe-490d-b86c-4e00c09e0b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2a58ad2-d14f-494a-9b01-63993ceb7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = [\"I like air.\", \"I like the sun.\", \"He likes the sun.\"]\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tk_seqs = [tokenizer(s) + [EOS_TOKEN] for s in seqs]\n",
    "# all_tks = reduce(lambda x, y: x + y, tk_seqs)\n",
    "# all_tks = [all_tks]  # build_vocab_from_iterator requires each item to be iterator\n",
    "vocab = build_vocab_from_iterator(tk_seqs, specials=SPECIAL_TOKENS)\n",
    "# TODO: consider whether to include this\n",
    "# vocab.set_default_index(-1)\n",
    "\n",
    "# numericalized sequences; must be tensor to be input to pad_sequence\n",
    "nz_seqs = [th.tensor(vocab(tk_seq)) for tk_seq in tk_seqs]\n",
    "nz_seqs_padded = pad_sequence(nz_seqs, padding_value=vocab[PAD_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6644192d-7e63-436f-8f63-2f0cd1096fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_glove = GloVe(name='6B', dim=embedding_dim, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7b6520c9-d57f-4a2a-acde-11886e44d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_embedding = True\n",
    "\n",
    "vocab_itos = vocab.get_itos()\n",
    "embedding_matrix = th.zeros((len(vocab_itos), embedding_dim))\n",
    "for i, s in enumerate(vocab_itos):\n",
    "    embedding_matrix[i, :] = embedding_glove[s]\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embedding, padding_idx=vocab[PAD_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe53dc-8948-47aa-8b40-ce6ff6748a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* create_glove_embedding(embedding_dim, vocab, freeze=True, padding_idx=vocab[PAD_TOKEN])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c784992-a8a8-40f9-be32-4230e8efd85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725093e-8bc5-4dbe-bdb1-e361ff8dba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{dir_data}/train.csv\")\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "full_ds = DisasterTweetsDataset(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66abab15-d65e-4aa5-b012-42cff7537709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e6efc-0810-4a1f-8ece-80e30132b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_glove = GloVe(name='6B', dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce2cc8-a99f-423e-8367-3b7a0bfd000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_embedding = True\n",
    "\n",
    "vocab_itos = full_ds.vocab.get_itos()  # \"i to s\"\n",
    "embedding_matrix = th.zeros((len(vocab_itos), embedding_dim))\n",
    "for i, s in enumerate(vocab_itos):\n",
    "    embedding_matrix[i, :] = embedding_glove[s]\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embedding, padding_idx=full_ds.pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b0d11-6bf8-4a33-9792-1ab1ada4fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lengths_of_sequences(sequences: th.Tensor, eos_idx: int) -> th.Tensor:\n",
    "    \"\"\"\n",
    "    Finds index of eos_idx (i.e. the length of seq). Assumes input is padded sequence, \n",
    "    and input shape is (N, max_L), where N is num sequences, and max_L is max length of seq.\n",
    "    Will return a 1-D tensor that indicates the lengths of each seq in the batch. Asserts that \n",
    "    there exists exactly 1 eos_idx occurrence in each seq.\n",
    "    \"\"\"\n",
    "    # assert len(sequences.shape) == 2\n",
    "    num_seq = sequences.shape[0]\n",
    "    max_length = sequences.shape[1]\n",
    "\n",
    "    indices_first_dim, indices_second_dim = (sequences == eos_idx).nonzero(as_tuple=True)\n",
    "    # assert (indices_first_dim == th.arange(num_seq)).all()\n",
    "\n",
    "    return indices_second_dim\n",
    "\n",
    "\n",
    "def test_lengths_of_sequences():\n",
    "    x = th.zeros([4, 10])\n",
    "    x[0, 5] = 1\n",
    "    x[1, 4] = 1\n",
    "    x[2, 7] = 1\n",
    "    x[3, 9] = 1\n",
    "    res = lengths_of_sequences(x, 1)\n",
    "    assert (res == th.tensor([5, 4, 7, 9])).all()\n",
    "test_lengths_of_sequences()\n",
    "\n",
    "\n",
    "class LitVanillaRNN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, embedding: nn.Embedding, hidden_size: int = 128, lr: float = 1e-3, dropout: float = 0.5,\n",
    "        num_layers: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # TODO: try using dropout in rnn\n",
    "        # TODO: try using bidirectional in rnn\n",
    "        self.rnn = nn.RNN(\n",
    "            self.embedding.embedding_dim, self.hidden_size, batch_first=True, dropout=self.dropout,\n",
    "            num_layers=self.num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, 2)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.acc = torchmetrics.Accuracy()\n",
    "    \n",
    "    def forward(self, x: th.Tensor) -> th.Tensor:\n",
    "        assert len(x) == 3\n",
    "        batch_text = x[2]  # index points to \"text\" part of data (not \"keyword\" or \"location\")\n",
    "        # assert batch_text.shape == (batch_size, train_ds[0][0][2].shape[0])\n",
    "        \n",
    "        lengths = lengths_of_sequences(batch_text, full_ds.eos_idx)\n",
    "        lengths = lengths.cpu()  # need to be on cpu for pack_padded_sequence to work\n",
    "        batch_text_embedded = self.embedding(batch_text)\n",
    "        packed = pack_padded_sequence(batch_text_embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        # packed.type_as(batch_text)\n",
    "        # TODO: try usng a randomly generated hidden state (instead of the zero vector default)\n",
    "        _, h_n = self.rnn(packed)\n",
    "        # assert h_n.shape == (1, batch_size, self.hidden_size)\n",
    "        x = h_n[-1, :, :]\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = self.log_softmax(x)\n",
    "        # assert x.shape == (batch_size, 2)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        x, y = batch\n",
    "        predicted = self(x)\n",
    "        loss = F.nll_loss(predicted, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", self.acc(predicted, y))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        x, y = batch\n",
    "        predicted = self(x)\n",
    "        loss = F.nll_loss(predicted, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", self.acc(predicted, y))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        x, y = batch\n",
    "        predicted = self(x)\n",
    "        loss = F.nll_loss(predicted, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_acc\", self.acc(predicted, y))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01f8cb-a8e4-4fb0-8e28-14a15d06c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrnn = LitVanillaRNN(embedding, dropout=0.5, lr=1e-4, num_layers=3)\n",
    "trainer = pl.Trainer(auto_lr_find=True, gpus=1)\n",
    "lr_finder = trainer.tuner.lr_find(vrnn, train_dl, max_lr=1e-1, num_training=1000)\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "# lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d58da-53ab-4e1e-b4c5-ec60c8d1d754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db255e41-e7a9-47f2-afc8-e98052e0f70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376f5d3f-79ab-4719-b105-83411a16bf75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e619c535-48cb-47d5-a1bd-dcd6b030505c",
   "metadata": {},
   "source": [
    "# 3. Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341461b1-fc50-41a7-8fa6-20995d24804a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e49db-fa8e-4030-ad72-a93d4484746f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e2f0c3e-924b-43f4-b178-4c4213c5d322",
   "metadata": {},
   "source": [
    "# Misc: code for future models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0906547-ee26-4282-9cfc-ba1ce22fbcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8adc6-41b7-4d16-98ad-c30446be2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for when you're building vocab from training dataset\n",
    "\n",
    "seqs = [\"I like air.\", \"I like the sun.\", \"He likes the sun.\"]\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tk_seqs = [tokenizer(s) + [EOS_TOKEN] for s in seqs]\n",
    "# all_tks = reduce(lambda x, y: x + y, tk_seqs)\n",
    "# all_tks = [all_tks]  # build_vocab_from_iterator requires each item to be iterator\n",
    "vocab = build_vocab_from_iterator(tk_seqs, specials=SPECIAL_TOKENS)\n",
    "# TODO: consider whether to include this\n",
    "# vocab.set_default_index(-1)\n",
    "\n",
    "# numericalized sequences; must be tensor to be input to pad_sequence\n",
    "nz_seqs = [th.tensor(vocab(tk_seq)) for tk_seq in tk_seqs]\n",
    "nz_seqs_padded = pad_sequence(nz_seqs, padding_value=vocab[PAD_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea1ac9-8f83-4935-b7f6-0849dbf884d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f5bc546-42e8-4a66-9a15-f6337e0e8a14",
   "metadata": {},
   "source": [
    "Parse and clean data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec1a9f4-5047-4a81-8332-dcc73b34013e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reviews_Office_Products_5.json.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fl/9k8dsknn4b1_q6cnfjypzvrw0000gn/T/ipykernel_99390/3624614200.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reviews_Office_Products_5.json.gz\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reviews_Office_Products_5.json.gz'"
     ]
    }
   ],
   "source": [
    "# Use example code from\n",
    "# https://colab.research.google.com/drive/1Zv6MARGQcrBbLHyjPVVMZVnRWsRnVMpV#scrollTo=7igYuRaV4bF7\n",
    "\n",
    "data = []\n",
    "with gzip.open(\"reviews_Office_Products_5.json.gz\") as f:\n",
    "    for l in f:\n",
    "        data.append(json.loads(l.strip()))\n",
    "    \n",
    "# total length of list, this number equals total number of products\n",
    "print(f\"Total num items in dataset: {len(data)}\")\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df = df.loc[:, [\"reviewText\", \"overall\"]]\n",
    "df = df.rename(columns={\"reviewText\": \"review_text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3862bade-bdb9-489f-8e95-563a2bd94530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual mapping from \"overall rating\" to \"sentiment\"\n",
    "MIN_RATING = 1\n",
    "MAX_RATING = 5\n",
    "NEUTRAL_RATING = 3.5\n",
    "def rating_to_sentiment(rating, neutral=NEUTRAL_RATING):\n",
    "    \"\"\"\n",
    "    Shifts scale to have `neutral` correspond to 0 sentiment, 1 to correspond to -1 sentiment,\n",
    "    and 5 to correspond to +1 sentiment. The mapping is a piecewise function, linear from min rating to neutral,\n",
    "    and again linear from neutral rating to max.\n",
    "    \"\"\"\n",
    "    assert MIN_RATING <= rating <= MAX_RATING\n",
    "    if rating <= neutral:\n",
    "        d = neutral - MIN_RATING\n",
    "        return -1 + (rating - MIN_RATING) / d\n",
    "    else:\n",
    "        d = MAX_RATING - neutral\n",
    "        return (rating - neutral) / d\n",
    "\n",
    "assert rating_to_sentiment(3.5) == 0\n",
    "assert rating_to_sentiment(5) == 1\n",
    "assert rating_to_sentiment(1) == -1\n",
    "assert rating_to_sentiment(3, neutral=3) == 0\n",
    "assert rating_to_sentiment(4, neutral=3) == 0.5\n",
    "assert rating_to_sentiment(2, neutral=3) == -0.5\n",
    "assert rating_to_sentiment(4) < rating_to_sentiment(4, neutral=3)\n",
    "assert rating_to_sentiment(2) < rating_to_sentiment(2, neutral=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd528630-6302-4965-ab36-53908b36c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"] = df.overall.apply(rating_to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2193872-d0bb-458c-9d8e-19f9f001beaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I bought my first HP12C in about 1984 or so, a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHY THIS BELATED REVIEW? I feel very obliged t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have an HP 48GX that has been kicking for mo...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've started doing more finance stuff recently...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For simple calculations and discounted cash fl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  overall  sentiment\n",
       "0  I bought my first HP12C in about 1984 or so, a...      5.0        1.0\n",
       "1  WHY THIS BELATED REVIEW? I feel very obliged t...      5.0        1.0\n",
       "2  I have an HP 48GX that has been kicking for mo...      2.0       -0.6\n",
       "3  I've started doing more finance stuff recently...      5.0        1.0\n",
       "4  For simple calculations and discounted cash fl...      5.0        1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
