{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526c0551-ead9-4c9f-acfb-cb1daded3236",
   "metadata": {},
   "source": [
    "Build training pipeline for models with different hparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef868328-09a5-48c1-b0a7-0c71427f3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torchtext.vocab import vocab, Vocab, GloVe, build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "from typing import Callable, List, Tuple, Iterable, Dict\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_contour\n",
    "from optuna.importance import get_param_importances\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "import wandb\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9655ed24-60f3-49c7-bc56-c3848cf257be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1842ea9c-9dfe-43c2-9fd3-58a65f21bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "SPECIAL_TOKENS = (PAD_TOKEN, EOS_TOKEN, UNK_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a477ad6-d4d7-47f7-931c-7aff23d6f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf47c34-a68d-42f9-922c-20acc976ef8d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638f0aae-356a-4dc7-b08f-c513282b8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nums_from_fractions(total: int, fractions: Tuple[float]) -> Tuple[int]:\n",
    "    \"\"\"\n",
    "    :param fractions: fractions of the total number. One elem must be -1, \n",
    "        which denotes \"remaining\"\n",
    "    \"\"\"\n",
    "    assert fractions.count(-1) == 1, (\n",
    "        \"Must have exactly one occurence of -1 to denote a fraction of 'remaining' items\"\n",
    "    )\n",
    "    nums = [int(total * f) if f != -1 else 0 for f in fractions]\n",
    "    idx_remaining = fractions.index(-1)\n",
    "    nums[idx_remaining] = total - sum(nums)\n",
    "    assert all([elem >= 0 for elem in nums])\n",
    "    return tuple(nums)\n",
    "\n",
    "assert nums_from_fractions(100, [0.7, 0.3, -1]) == (70, 30, 0)\n",
    "assert nums_from_fractions(100, [0.7, 0.155, -1]) == (70, 15, 15)\n",
    "assert nums_from_fractions(100, [0.7, 0, -1]) == (70, 0, 30)\n",
    "# tested that these lines raise error, as expected: \n",
    "# nums_from_fractions(100, [0.7, 0.3, -2])\n",
    "# nums_from_fractions(100, [0.7, 0.5, -1])\n",
    "\n",
    "def build_vocab_from_texts(\n",
    "    texts: Iterable[str], tokenizer: Callable, specials=SPECIAL_TOKENS, \n",
    "    unk_token=UNK_TOKEN, **kwargs\n",
    ") -> Vocab:\n",
    "    tk_seqs = [tokenizer(s) for s in tqdm(texts)]\n",
    "    voc = build_vocab_from_iterator(tk_seqs, specials=specials, **kwargs)\n",
    "    voc.set_default_index(voc[unk_token])\n",
    "    return voc\n",
    "\n",
    "def seqs_from_texts(\n",
    "    texts: List[str], tokenizer: Callable, voc: Vocab, pad_token=PAD_TOKEN\n",
    ") -> th.Tensor:\n",
    "    \"\"\"\n",
    "    Returns padded sequences (numericalized texts), in tensor form\n",
    "    \"\"\"\n",
    "    nz_texts = [th.tensor(voc(tokenizer(text))) for text in texts]\n",
    "    seqs = pad_sequence(nz_texts, padding_value=voc[pad_token])\n",
    "    return seqs\n",
    "\n",
    "def count_oov_rate(\n",
    "    seqs: Iterable[th.Tensor], voc: Vocab, unk_token=UNK_TOKEN, \n",
    "    pad_token=PAD_TOKEN\n",
    ") -> float:\n",
    "    num_oov = 0\n",
    "    num_tokens = 0\n",
    "    for i, item in enumerate(seqs):\n",
    "        num_oov += th.sum(item == voc[unk_token]).item()\n",
    "        num_tokens += th.sum(item != voc[pad_token]).item()\n",
    "    return num_oov / num_tokens\n",
    "\n",
    "def glove_voc_and_embedding(\n",
    "    embedding_dim: int, \n",
    "    glove_embedding_params: Dict,\n",
    "    pad_token=PAD_TOKEN, \n",
    "    eos_token=EOS_TOKEN, \n",
    "    unk_token=UNK_TOKEN\n",
    ") -> Tuple[Vocab, Embedding]:\n",
    "    embedding_vecs = GloVe(name=glove_embedding_params[\"name\"], dim=embedding_dim)\n",
    "\n",
    "    embedding_dict = OrderedDict()\n",
    "    embedding_dict.update({pad_token: 1})\n",
    "    embedding_dict.update({eos_token: 1})\n",
    "    embedding_dict.update({unk_token: 1})\n",
    "    embedding_dict.update(embedding_vecs.stoi)\n",
    "    # min_freq=0 is a hack to read in the 0th token from embedding_vecs.stoi\n",
    "    voc = vocab(embedding_dict, min_freq=0)\n",
    "    voc.set_default_index(voc[unk_token])\n",
    "\n",
    "    embedding = Embedding.from_pretrained(\n",
    "        embedding_vecs.vectors, freeze=glove_embedding_params[\"freeze_embedding\"], \n",
    "        padding_idx=voc[pad_token]\n",
    "    )\n",
    "    \n",
    "    return voc, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8389f5-f912-4de2-bbc0-fddcd723e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: Callable, voc: Vocab) -> None:\n",
    "        assert \"text\" in df.columns\n",
    "        assert \"target\" in df.columns\n",
    "        self.tokenizer = tokenizer\n",
    "        self.voc = voc\n",
    "        \n",
    "        nz_texts = []  # numericalized_texts\n",
    "        seq_lengths = []  # sequence lengths\n",
    "        for text in tqdm(df.text):\n",
    "            nz_text = th.tensor(self.voc(self.tokenizer(text)))\n",
    "            nz_texts.append(nz_text)\n",
    "            seq_lengths.append(len(nz_text))\n",
    "        \n",
    "        # shape of x is: T x B, where T is length of longest seq, B is batch size\n",
    "        self.seqs = pad_sequence(nz_texts, padding_value=self.voc[PAD_TOKEN])\n",
    "        self.seq_lengths = th.tensor(seq_lengths)\n",
    "        self.targets = th.tensor(df.target.values).float()\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, i: int) -> Tuple[Tuple[th.Tensor, int], float]:\n",
    "        seq = self.seqs[:, i]\n",
    "        seq_length = self.seq_lengths[i]\n",
    "        targets = self.targets[i]\n",
    "        return (seq, seq_length), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d977cb84-4573-4a84-9de7-96c7844bf659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding: nn.Embedding, \n",
    "        hidden_size: int = 128, \n",
    "        num_layers: int = 1,\n",
    "        lr: float = 1e-3, \n",
    "        dropout: float = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['embedding'])\n",
    "\n",
    "        # TODO: try using bidirectional in rnn\n",
    "        self.rnn = nn.RNN(\n",
    "            self.embedding.embedding_dim, self.hidden_size, batch_first=True, \n",
    "            dropout=self.dropout, num_layers=self.num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x: List[th.Tensor]) -> th.Tensor:\n",
    "        assert len(x) == 2\n",
    "        seqs, seq_lengths = x\n",
    "        embedded = self.embedding(seqs)\n",
    "        packed = pack_padded_sequence(\n",
    "            embedded, seq_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # TODO: try usng a randomly generated initial hidden state \n",
    "        # (instead of the zero vector default)\n",
    "        _, h_n = self.rnn(packed)\n",
    "        \n",
    "        assert h_n.shape[0], h_n.shape[2] == (1, self.hidden_size)\n",
    "        \n",
    "        x = h_n[-1, :, :]\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"test\")\n",
    "    \n",
    "    def generalized_step(self, batch: th.Tensor, batch_idx: int, label: str) -> th.Tensor:\n",
    "        x, y = batch\n",
    "        predicted = self(x).squeeze(1)\n",
    "        loss = F.mse_loss(predicted, y)\n",
    "        self.log(f\"{label}_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "def construct_model(model_config: Dict):\n",
    "    if model_config[\"model_arch\"] == \"VanillaRNN\":\n",
    "        params = list(inspect.signature(VanillaRNN).parameters)\n",
    "        relevant_params = [p for p in params if p != \"embedding\"]\n",
    "        hparams = {k: v for k, v in model_config.items() if (k in relevant_params)}\n",
    "        return VanillaRNN(embedding, **hparams)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "484b720b-6a45-4d3d-93f4-9970bf930924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_disaster_tweets.csv\")\n",
    "df = df[[\"text\", \"target\"]].reset_index(drop=True)\n",
    "\n",
    "data_config = dict(\n",
    "    fractions=[0.7, 0.15, -1],\n",
    "    batch_size=64\n",
    ")\n",
    "embedding_dim = 100\n",
    "glove_embedding_config = None  # contains keys: name, freeze_embedding\n",
    "\n",
    "model_config = dict(\n",
    "    model_arch=\"VanillaRNN\",\n",
    "    num_layers=3,\n",
    "    hidden_size=128,\n",
    "    lr=1e-3,\n",
    "    dropout=0.5,\n",
    "    layer_norm=False,\n",
    "    residual_connections=False,\n",
    "    loss_fn=\"MSELoss\",\n",
    ")\n",
    "\n",
    "wandb_config = dict(\n",
    "    project='scratch', \n",
    "    log_model=False\n",
    ")\n",
    "\n",
    "train_config = dict(\n",
    "    max_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d75ea87-b06f-46d3-9463-f60cc34ba8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 5329/5329 [00:01<00:00, 3932.66it/s]\n",
      "100%|████████████████████████████████████████████████████████| 5329/5329 [00:00<00:00, 12927.25it/s]\n",
      "100%|████████████████████████████████████████████████████████| 1141/1141 [00:00<00:00, 13577.23it/s]\n",
      "100%|████████████████████████████████████████████████████████| 1143/1143 [00:00<00:00, 14668.14it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = {}\n",
    "texts[\"train\"], texts[\"val\"], texts[\"test\"] = random_split(\n",
    "    df.text, nums_from_fractions(len(df.text), data_config[\"fractions\"])\n",
    ")\n",
    "\n",
    "if glove_embedding_config:\n",
    "    voc, embedding = glove_voc_and_embedding(embedding_dim, glove_embedding_config)\n",
    "else:\n",
    "    voc = build_vocab_from_texts(texts[\"train\"], tokenizer)\n",
    "    embedding = Embedding(len(voc), embedding_dim, padding_idx=voc[PAD_TOKEN])\n",
    "    \n",
    "oov_rates = {}\n",
    "dls = {}  # dataloaders\n",
    "for label in [\"train\", \"val\", \"test\"]:\n",
    "    oov_rates[label] = count_oov_rate(seqs_from_texts(texts[label], tokenizer, voc), voc)\n",
    "    \n",
    "    ds = TextDataset(df.iloc[texts[label].indices], tokenizer, voc)\n",
    "    dls[label] = DataLoader(ds, batch_size=data_config[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f94534f-5acb-414b-8cf8-2377ba970eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = construct_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae483ebe-ce6d-4634-b1be-063f06c64731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 2.1 M \n",
      "1 | rnn       | RNN       | 95.5 K\n",
      "2 | fc        | Linear    | 129   \n",
      "3 | sigmoid   | Sigmoid   | 0     \n",
      "----------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.780     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennis/repos/sentimental/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:655: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/dennis/repos/sentimental/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/dennis/repos/sentimental/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45223401e2f6408e8539efa79790f953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37024... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.08MB of 0.08MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>▇▆██▆▆▅▅▃▄▃▃▃▃▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>▄▂▂▁▄▅█▇██</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_loss</td><td>0.08755</td></tr><tr><td>trainer/global_step</td><td>839</td></tr><tr><td>val_loss</td><td>0.23524</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earthy-wave-7</strong>: <a href=\"https://wandb.ai/dennisfeng/scratch/runs/2xsv8995\" target=\"_blank\">https://wandb.ai/dennisfeng/scratch/runs/2xsv8995</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220115_190004-2xsv8995/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = WandbLogger(**wandb_config)\n",
    "logger.watch(model, log=\"all\")\n",
    "trainer = pl.Trainer(max_epochs=train_config[\"max_epochs\"], logger=logger)\n",
    "trainer.fit(model, dls[\"train\"], dls[\"val\"])\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91faa1b5-0006-4466-a75d-9a40ad9c720e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aafa3b5-0f1b-4ab9-bda2-016fe2c75076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526850-c70a-442d-9858-bbbb43fc374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "907d08d1-cdfc-4ae2-9934-fe40101b7e92",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea21642-29ae-47be-ba03-4f6075461181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09c7e0-ec93-4602-98c3-53cd0584a0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
