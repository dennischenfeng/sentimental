{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526c0551-ead9-4c9f-acfb-cb1daded3236",
   "metadata": {},
   "source": [
    "Build training pipeline for models with different hparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef868328-09a5-48c1-b0a7-0c71427f3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torchtext.vocab import vocab, Vocab, GloVe, build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "from typing import Callable, List, Tuple, Iterable, Dict\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_contour\n",
    "from optuna.importance import get_param_importances\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "import wandb\n",
    "import inspect\n",
    "\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9655ed24-60f3-49c7-bc56-c3848cf257be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1842ea9c-9dfe-43c2-9fd3-58a65f21bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "SPECIAL_TOKENS = (PAD_TOKEN, EOS_TOKEN, UNK_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf47c34-a68d-42f9-922c-20acc976ef8d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "638f0aae-356a-4dc7-b08f-c513282b8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nums_from_fractions(total: int, fractions: Tuple[float]) -> Tuple[int]:\n",
    "    \"\"\"\n",
    "    :param fractions: fractions of the total number. One elem must be -1, \n",
    "        which denotes \"remaining\"\n",
    "    \"\"\"\n",
    "    assert fractions.count(-1) == 1, (\n",
    "        \"Must have exactly one occurence of -1 to denote a fraction of 'remaining' items\"\n",
    "    )\n",
    "    nums = [int(total * f) if f != -1 else 0 for f in fractions]\n",
    "    idx_remaining = fractions.index(-1)\n",
    "    nums[idx_remaining] = total - sum(nums)\n",
    "    assert all([elem >= 0 for elem in nums])\n",
    "    return tuple(nums)\n",
    "\n",
    "assert nums_from_fractions(100, [0.7, 0.3, -1]) == (70, 30, 0)\n",
    "assert nums_from_fractions(100, [0.7, 0.155, -1]) == (70, 15, 15)\n",
    "assert nums_from_fractions(100, [0.7, 0, -1]) == (70, 0, 30)\n",
    "# tested that these lines raise error, as expected: \n",
    "# nums_from_fractions(100, [0.7, 0.3, -2])\n",
    "# nums_from_fractions(100, [0.7, 0.5, -1])\n",
    "\n",
    "def build_vocab_from_texts(\n",
    "    texts: Iterable[str], tokenizer: Callable, specials=SPECIAL_TOKENS, \n",
    "    unk_token=UNK_TOKEN, **kwargs\n",
    ") -> Vocab:\n",
    "    tk_seqs = [tokenizer(s) for s in tqdm(texts)]\n",
    "    voc = build_vocab_from_iterator(tk_seqs, specials=specials, **kwargs)\n",
    "    voc.set_default_index(voc[unk_token])\n",
    "    return voc\n",
    "\n",
    "def seqs_from_texts(\n",
    "    texts: List[str], tokenizer: Callable, voc: Vocab, pad_token=PAD_TOKEN\n",
    ") -> th.Tensor:\n",
    "    \"\"\"\n",
    "    Returns padded sequences (numericalized texts), in tensor form\n",
    "    \"\"\"\n",
    "    nz_texts = [th.tensor(voc(tokenizer(text))) for text in texts]\n",
    "    seqs = pad_sequence(nz_texts, padding_value=voc[pad_token])\n",
    "    return seqs\n",
    "\n",
    "def count_oov_rate(\n",
    "    seqs: Iterable[th.Tensor], voc: Vocab, unk_token=UNK_TOKEN, \n",
    "    pad_token=PAD_TOKEN\n",
    ") -> float:\n",
    "    num_oov = 0\n",
    "    num_tokens = 0\n",
    "    for i, item in enumerate(seqs):\n",
    "        num_oov += th.sum(item == voc[unk_token]).item()\n",
    "        num_tokens += th.sum(item != voc[pad_token]).item()\n",
    "    return num_oov / num_tokens\n",
    "\n",
    "def glove_voc_and_embedding(\n",
    "    embedding_dim: int, \n",
    "    glove_embedding_params: Dict,\n",
    "    pad_token=PAD_TOKEN, \n",
    "    eos_token=EOS_TOKEN, \n",
    "    unk_token=UNK_TOKEN\n",
    ") -> Tuple[Vocab, Embedding]:\n",
    "    embedding_vecs = GloVe(name=glove_embedding_params[\"name\"], dim=embedding_dim)\n",
    "\n",
    "    embedding_dict = OrderedDict()\n",
    "    embedding_dict.update({pad_token: 1})\n",
    "    embedding_dict.update({eos_token: 1})\n",
    "    embedding_dict.update({unk_token: 1})\n",
    "    embedding_dict.update(embedding_vecs.stoi)\n",
    "    # min_freq=0 is a hack to read in the 0th token from embedding_vecs.stoi\n",
    "    voc = vocab(embedding_dict, min_freq=0)\n",
    "    voc.set_default_index(voc[unk_token])\n",
    "\n",
    "    embedding = Embedding.from_pretrained(\n",
    "        embedding_vecs.vectors, freeze=glove_embedding_params[\"freeze_embedding\"], \n",
    "        padding_idx=voc[pad_token]\n",
    "    )\n",
    "    \n",
    "    return voc, embedding\n",
    "\n",
    "@lru_cache()\n",
    "def get_raw_data(name: str) -> pd.DataFrame:\n",
    "    if name == \"twitter_disaster\":\n",
    "        df = pd.read_csv(\"data/data_disaster_tweets.csv\")\n",
    "    elif name == \"twitter_sentiment140\":\n",
    "        df = pd.read_csv(\n",
    "            \"data/data_twitter_sentiment.csv\", header=None, encoding='latin-1'\n",
    "        )\n",
    "        df = df.rename(columns={0: \"target_raw\", 5: \"text\"})\n",
    "        df[\"target\"] = df.target_raw / 4\n",
    "    elif name == \"amazon_office_products\":\n",
    "        data = []\n",
    "        with gzip.open('data/data_reviews_Office_Products_5.json.gz') as f:\n",
    "            for l in tqdm(f):\n",
    "                data.append(json.loads(l.strip()))\n",
    "\n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        df = df.rename(columns={\"reviewText\": \"text\", \"overall\": \"target_raw\"})\n",
    "        df[\"target\"] = (df.target_raw - 1) / 4\n",
    "    elif name == \"imdb_reviews\":\n",
    "        basepath = \"data/stanford_movie_reviews/aclImdb/\"\n",
    "        labels = {'pos': 1, 'neg': 0}\n",
    "        df = pd.DataFrame()\n",
    "        for s in ('test', 'train'):\n",
    "            for l in ('pos', 'neg'):\n",
    "                path = os.path.join(basepath, s, l)\n",
    "                for file in tqdm(sorted(os.listdir(path))):\n",
    "                    with open(os.path.join(path, file),\n",
    "                              'r', encoding='utf-8') as infile:\n",
    "                        txt = infile.read()\n",
    "                    df = df.append([[txt, labels[l]]],\n",
    "                                   ignore_index=True)\n",
    "        df.columns = ['text', 'target']\n",
    "    else: \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return df[[\"text\", \"target\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d8389f5-f912-4de2-bbc0-fddcd723e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: Callable, voc: Vocab) -> None:\n",
    "        assert \"text\" in df.columns\n",
    "        assert \"target\" in df.columns\n",
    "        self.tokenizer = tokenizer\n",
    "        self.voc = voc\n",
    "        \n",
    "        nz_texts = []  # numericalized_texts\n",
    "        seq_lengths = []  # sequence lengths\n",
    "        for text in tqdm(df.text):\n",
    "            nz_text = th.tensor(self.voc(self.tokenizer(text)))\n",
    "            nz_texts.append(nz_text)\n",
    "            seq_lengths.append(len(nz_text))\n",
    "        \n",
    "        # shape of x is: T x B, where T is length of longest seq, B is batch size\n",
    "        self.seqs = pad_sequence(nz_texts, padding_value=self.voc[PAD_TOKEN])\n",
    "        self.seq_lengths = th.tensor(seq_lengths)\n",
    "        self.targets = th.tensor(df.target.values).float()\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, i: int) -> Tuple[Tuple[th.Tensor, int], float]:\n",
    "        seq = self.seqs[:, i]\n",
    "        seq_length = self.seq_lengths[i]\n",
    "        targets = self.targets[i]\n",
    "        return (seq, seq_length), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d977cb84-4573-4a84-9de7-96c7844bf659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding: nn.Embedding, \n",
    "        hidden_size: int = 128, \n",
    "        num_layers: int = 1,\n",
    "        lr: float = 1e-3, \n",
    "        dropout: float = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['embedding'])\n",
    "\n",
    "        # TODO: try using bidirectional in rnn\n",
    "        self.rnn = nn.RNN(\n",
    "            self.embedding.embedding_dim, self.hidden_size, batch_first=True, \n",
    "            dropout=self.dropout, num_layers=self.num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x: List[th.Tensor]) -> th.Tensor:\n",
    "        assert len(x) == 2\n",
    "        seqs, seq_lengths = x\n",
    "        \n",
    "        # to work on GPU-enabled machine, need to explicitly set only \n",
    "        # the seq_lengths to cpu\n",
    "        seq_lengths = seq_lengths.to(\"cpu\")\n",
    "        \n",
    "        embedded = self.embedding(seqs)\n",
    "        packed = pack_padded_sequence(\n",
    "            embedded, seq_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # TODO: try usng a randomly generated initial hidden state \n",
    "        # (instead of the zero vector default)\n",
    "        _, h_n = self.rnn(packed)\n",
    "        \n",
    "        assert h_n.shape[0], h_n.shape[2] == (1, self.hidden_size)\n",
    "        \n",
    "        x = h_n[-1, :, :]\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"test\")\n",
    "    \n",
    "    def generalized_step(\n",
    "        self, batch: th.Tensor, batch_idx: int, label: str\n",
    "    ) -> th.Tensor:\n",
    "        x, y = batch\n",
    "        predicted = self(x).squeeze(1)\n",
    "        loss = F.mse_loss(predicted, y)\n",
    "        self.log(f\"{label}_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "def construct_model(model_config: Dict, embedding: Embedding):\n",
    "    if model_config[\"model_arch\"] == \"VanillaRNN\":\n",
    "        params = list(inspect.signature(VanillaRNN).parameters)\n",
    "        relevant_params = [p for p in params if p != \"embedding\"]\n",
    "        hparams = {k: v for k, v in model_config.items() if (k in relevant_params)}\n",
    "        return VanillaRNN(embedding, **hparams)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebfa778-9bf1-4c12-8590-d1231bd4c6ad",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d28b8f44-3e7c-4d13-b191-572c13087151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_run(run_config: Dict) -> None:\n",
    "    rc = run_config\n",
    "    \n",
    "    df = get_raw_data(rc[\"data_config\"][\"name\"])\n",
    "\n",
    "    texts = {}\n",
    "    texts[\"train\"], texts[\"val\"], texts[\"test\"] = random_split(\n",
    "        df.text, nums_from_fractions(len(df.text), rc[\"data_config\"][\"fractions\"])\n",
    "    )\n",
    "    \n",
    "    tokenizer = get_tokenizer(**rc[\"tokenizer_config\"])\n",
    "\n",
    "    if rc[\"glove_embedding_config\"]:\n",
    "        voc, embedding = glove_voc_and_embedding(\n",
    "            rc[\"embedding_dim\"], rc[\"glove_embedding_config\"]\n",
    "        )\n",
    "    else:\n",
    "        voc = build_vocab_from_texts(texts[\"train\"], tokenizer)\n",
    "        embedding = Embedding(len(voc), rc[\"embedding_dim\"], padding_idx=voc[PAD_TOKEN])\n",
    "\n",
    "    oov_rates = {}\n",
    "    dls = {}  # dataloaders\n",
    "    for label in [\"train\", \"val\", \"test\"]:\n",
    "        oov_rates[label] = count_oov_rate(\n",
    "            seqs_from_texts(texts[label], tokenizer, voc), voc\n",
    "        )\n",
    "\n",
    "        ds = TextDataset(df.iloc[texts[label].indices], tokenizer, voc)\n",
    "        shuffle = True if label == \"train\" else False\n",
    "        dls[label] = DataLoader(\n",
    "            ds, \n",
    "            batch_size=rc[\"data_config\"][\"batch_size\"], \n",
    "            shuffle=shuffle,\n",
    "            num_workers=rc[\"data_config\"][\"num_workers\"]\n",
    "        )\n",
    "\n",
    "    model = construct_model(rc[\"model_config\"], embedding)\n",
    "\n",
    "    logger = WandbLogger(**rc[\"wandb_config\"])\n",
    "    logger.watch(model, log=\"all\")\n",
    "\n",
    "    # log more stuff\n",
    "    wandb.log(dict(\n",
    "        run_config = wandb.Table(\n",
    "            columns=list(rc.keys()),\n",
    "            data=[list(rc.values())],\n",
    "        ),\n",
    "    ))\n",
    "    wandb.log(dict(\n",
    "        oov_rate_train=oov_rates[\"train\"],\n",
    "        oov_rate_val=oov_rates[\"val\"],\n",
    "        oov_rate_test=oov_rates[\"test\"],\n",
    "        voc_size=len(voc),\n",
    "    ))\n",
    "\n",
    "    trainer = pl.Trainer(logger=logger, **rc[\"trainer_config\"])\n",
    "    trainer.fit(model, dls[\"train\"], dls[\"val\"])\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b720b-6a45-4d3d-93f4-9970bf930924",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = dict(\n",
    "    data_config = dict(\n",
    "        name=\"twitter_sentiment140\",\n",
    "        fractions=[0.7, 0.15, -1],\n",
    "        batch_size=64,\n",
    "        num_workers=0,  # default is 0\n",
    "    ),\n",
    "    tokenizer_config = dict(\n",
    "        tokenizer=\"spacy\",\n",
    "        language=\"en_core_web_sm\"\n",
    "    ),\n",
    "    embedding_dim = 100,\n",
    "    glove_embedding_config = None,  # contains keys: name, freeze_embedding\n",
    "    model_config = dict(\n",
    "        model_arch=\"VanillaRNN\",\n",
    "        num_layers=3,\n",
    "        hidden_size=128,\n",
    "        lr=1e-3,\n",
    "        dropout=0.5,\n",
    "        layer_norm=False,\n",
    "        residual_connections=False,\n",
    "        loss_fn=\"MSELoss\",\n",
    "    ),\n",
    "    wandb_config = dict(\n",
    "        project='scratch', \n",
    "        log_model=False\n",
    "    ),\n",
    "    trainer_config = dict(\n",
    "        max_epochs=100,\n",
    "        gpus=None,\n",
    "    )\n",
    ")\n",
    "\n",
    "perform_run(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526850-c70a-442d-9858-bbbb43fc374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "907d08d1-cdfc-4ae2-9934-fe40101b7e92",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1175e-d852-479a-b8d2-76480800fab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
