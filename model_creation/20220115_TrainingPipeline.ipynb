{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526c0551-ead9-4c9f-acfb-cb1daded3236",
   "metadata": {},
   "source": [
    "Build training pipeline for models with different hparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188e33ab-fd40-4031-8d56-67ee318cfc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef868328-09a5-48c1-b0a7-0c71427f3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from typing import Callable, List, Tuple, Iterable, Dict, Type, Any\n",
    "from functools import reduce, lru_cache\n",
    "from collections import OrderedDict\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, PackedSequence\n",
    "from torchtext.vocab import vocab, Vocab, GloVe, build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_contour\n",
    "from optuna.importance import get_param_importances\n",
    "\n",
    "import wandb\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "SPECIAL_TOKENS = (PAD_TOKEN, UNK_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf47c34-a68d-42f9-922c-20acc976ef8d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638f0aae-356a-4dc7-b08f-c513282b8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nums_from_fractions(total: int, fractions: Tuple[float]) -> Tuple[int]:\n",
    "    \"\"\"\n",
    "    :param fractions: fractions of the total number. One elem must be -1, \n",
    "        which denotes \"remaining\"\n",
    "    \"\"\"\n",
    "    assert fractions.count(-1) == 1, (\n",
    "        \"Must have exactly one occurence of -1 to denote a fraction of 'remaining' items\"\n",
    "    )\n",
    "    nums = [int(total * f) if f != -1 else 0 for f in fractions]\n",
    "    idx_remaining = fractions.index(-1)\n",
    "    nums[idx_remaining] = total - sum(nums)\n",
    "    assert all([elem >= 0 for elem in nums])\n",
    "    return tuple(nums)\n",
    "\n",
    "assert nums_from_fractions(100, [0.7, 0.3, -1]) == (70, 30, 0)\n",
    "assert nums_from_fractions(100, [0.7, 0.155, -1]) == (70, 15, 15)\n",
    "assert nums_from_fractions(100, [0.7, 0, -1]) == (70, 0, 30)\n",
    "# tested that these lines raise error, as expected: \n",
    "# nums_from_fractions(100, [0.7, 0.3, -2])\n",
    "# nums_from_fractions(100, [0.7, 0.5, -1])\n",
    "\n",
    "def build_vocab_from_texts(\n",
    "    texts: Iterable[str], tokenizer: Callable, specials=SPECIAL_TOKENS, \n",
    "    unk_token=UNK_TOKEN, **kwargs\n",
    ") -> Vocab:\n",
    "    tk_seqs = [tokenizer(s) for s in tqdm(texts)]\n",
    "    voc = build_vocab_from_iterator(tk_seqs, specials=specials, **kwargs)\n",
    "    voc.set_default_index(voc[unk_token])\n",
    "    return voc\n",
    "\n",
    "def seqs_from_texts(\n",
    "    texts: List[str], tokenizer: Callable, voc: Vocab, pad_token=PAD_TOKEN\n",
    ") -> th.Tensor:\n",
    "    \"\"\"\n",
    "    Returns padded sequences (numericalized texts), in tensor form\n",
    "    \"\"\"\n",
    "    nz_texts = [th.tensor(voc(tokenizer(text))) for text in texts]\n",
    "    seqs = pad_sequence(nz_texts, padding_value=voc[pad_token])\n",
    "    return seqs\n",
    "\n",
    "def count_oov_rate(\n",
    "    seqs: Iterable[th.Tensor], \n",
    "    voc: Vocab, \n",
    "    unk_token=UNK_TOKEN, \n",
    "    pad_token=PAD_TOKEN\n",
    ") -> float:\n",
    "    num_oov = 0\n",
    "    num_tokens = 0\n",
    "    for i, item in enumerate(seqs):\n",
    "        num_oov += th.sum(item == voc[unk_token]).item()\n",
    "        num_tokens += th.sum(item != voc[pad_token]).item()\n",
    "    return num_oov / num_tokens\n",
    "\n",
    "def glove_voc_and_embedding(\n",
    "    embedding_dim: int, \n",
    "    glove_embedding_params: Dict,\n",
    "    pad_token=PAD_TOKEN,\n",
    "    unk_token=UNK_TOKEN\n",
    ") -> Tuple[Vocab, Embedding]:\n",
    "    embedding_vecs = GloVe(name=glove_embedding_params[\"name\"], dim=embedding_dim)\n",
    "\n",
    "    embedding_dict = OrderedDict()\n",
    "    embedding_dict.update({pad_token: 1})\n",
    "    embedding_dict.update({unk_token: 1})\n",
    "    embedding_dict.update(embedding_vecs.stoi)\n",
    "    # min_freq=0 is a hack to read in the 0th token from embedding_vecs.stoi\n",
    "    voc = vocab(embedding_dict, min_freq=0)\n",
    "    voc.set_default_index(voc[unk_token])\n",
    "\n",
    "    embedding = Embedding.from_pretrained(\n",
    "        embedding_vecs.vectors, freeze=glove_embedding_params[\"freeze_embedding\"], \n",
    "        padding_idx=voc[pad_token]\n",
    "    )\n",
    "    \n",
    "    return voc, embedding\n",
    "\n",
    "@lru_cache()\n",
    "def get_raw_data(name: str) -> pd.DataFrame:\n",
    "    if name == \"twitter_disaster\":\n",
    "        df = pd.read_csv(\"data/data_disaster_tweets.csv\")\n",
    "    elif name == \"twitter_sentiment140\":\n",
    "        df = pd.read_csv(\n",
    "            \"data/data_twitter_sentiment.csv\", header=None, encoding='latin-1'\n",
    "        )\n",
    "        df = df.rename(columns={0: \"target_raw\", 5: \"text\"})\n",
    "        df[\"target\"] = df.target_raw / 4\n",
    "    elif name == \"twitter_sentiment140_random_small\":\n",
    "        df = pd.read_csv(\n",
    "            \"data/data_twitter_sentiment.csv\", header=None, encoding='latin-1'\n",
    "        )\n",
    "        df = df.rename(columns={0: \"target_raw\", 5: \"text\"})\n",
    "        df[\"target\"] = df.target_raw / 4\n",
    "        \n",
    "        random_indices = np.random.choice(\n",
    "            df.shape[0],\n",
    "            int(30e3),\n",
    "            replace=False\n",
    "        )\n",
    "        df = df.iloc[random_indices, :]\n",
    "    elif name == \"amazon_office_products\":\n",
    "        data = []\n",
    "        with gzip.open('data/data_reviews_Office_Products_5.json.gz') as f:\n",
    "            for l in tqdm(f):\n",
    "                data.append(json.loads(l.strip()))\n",
    "\n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        df = df.rename(columns={\"reviewText\": \"text\", \"overall\": \"target_raw\"})\n",
    "        df[\"target\"] = (df.target_raw - 1) / 4\n",
    "    elif name == \"imdb_reviews\":\n",
    "        basepath = \"data/stanford_movie_reviews/aclImdb/\"\n",
    "        labels = {'pos': 1, 'neg': 0}\n",
    "        df = pd.DataFrame()\n",
    "        for s in ('test', 'train'):\n",
    "            for l in ('pos', 'neg'):\n",
    "                path = os.path.join(basepath, s, l)\n",
    "                for file in tqdm(sorted(os.listdir(path))):\n",
    "                    with open(os.path.join(path, file),\n",
    "                              'r', encoding='utf-8') as infile:\n",
    "                        txt = infile.read()\n",
    "                    df = df.append([[txt, labels[l]]],\n",
    "                                   ignore_index=True)\n",
    "        df.columns = ['text', 'target']\n",
    "    else: \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return df[[\"text\", \"target\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d8389f5-f912-4de2-bbc0-fddcd723e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: Callable, voc: Vocab) -> None:\n",
    "        assert \"text\" in df.columns\n",
    "        assert \"target\" in df.columns\n",
    "        self.tokenizer = tokenizer\n",
    "        self.voc = voc\n",
    "        \n",
    "        nz_texts = []  # numericalized_texts\n",
    "        seq_lengths = []  # sequence lengths\n",
    "        for text in tqdm(df.text):\n",
    "            nz_text = th.tensor(self.voc(self.tokenizer(text)))\n",
    "            nz_texts.append(nz_text)\n",
    "            seq_lengths.append(len(nz_text))\n",
    "        \n",
    "        # shape of x is: T x B, where T is length of longest seq, B is batch size\n",
    "        self.seqs = pad_sequence(nz_texts, padding_value=self.voc[PAD_TOKEN])\n",
    "        self.seq_lengths = th.tensor(seq_lengths)\n",
    "        self.targets = th.tensor(df.target.values).float()\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, i: int) -> Tuple[Tuple[th.Tensor, int], float]:\n",
    "        seq = self.seqs[:, i]\n",
    "        seq_length = self.seq_lengths[i]\n",
    "        targets = self.targets[i]\n",
    "        return (seq, seq_length), targets\n",
    "    \n",
    "class TextDistilbertDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, hf_tokenizer: Callable) -> None:\n",
    "        assert \"text\" in df.columns\n",
    "        assert \"target\" in df.columns\n",
    "        \n",
    "        tk_output = hf_tokenizer(\n",
    "            list(df.text), \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True\n",
    "        )\n",
    "        self.seqs = tk_output[\"input_ids\"]\n",
    "        self.attention_masks = tk_output[\"attention_mask\"]\n",
    "        self.targets = th.tensor(df.target.values).float()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, i: int) -> Tuple[Tuple[th.Tensor, th.Tensor], float]:\n",
    "        seq = self.seqs[i, :]\n",
    "        attention_mask = self.attention_masks[i, :]\n",
    "        targets = self.targets[i]\n",
    "        return (seq, attention_mask), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d977cb84-4573-4a84-9de7-96c7844bf659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedTextRNN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        rnn_cls: Type,\n",
    "        embedding: nn.Embedding, \n",
    "        hidden_size: int = 128, \n",
    "        num_layers: int = 1,\n",
    "        lr: float = 1e-3, \n",
    "        dropout: float = 0.5\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.rnn_cls = rnn_cls\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['embedding'])\n",
    "\n",
    "        # TODO: try using bidirectional in rnn\n",
    "        self.rnn = self.rnn_cls(\n",
    "            self.embedding.embedding_dim, self.hidden_size, batch_first=True, \n",
    "            dropout=self.dropout, num_layers=self.num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x: List[th.Tensor]) -> th.Tensor:\n",
    "        assert len(x) == 2\n",
    "        seqs, seq_lengths = x\n",
    "        \n",
    "        # to work on GPU-enabled machine, need to explicitly set only \n",
    "        # the seq_lengths to cpu\n",
    "        seq_lengths = seq_lengths.to(\"cpu\")\n",
    "        \n",
    "        embedded = self.embedding(seqs)\n",
    "        packed = pack_padded_sequence(\n",
    "            embedded, seq_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # TODO: try usng a randomly generated initial hidden state \n",
    "        # (instead of the zero vector default)\n",
    "        rnn_outputs = self.rnn(packed)\n",
    "        h_n = self.hidden_state_from_rnn_outputs(rnn_outputs)\n",
    "        \n",
    "        assert h_n.shape[0], h_n.shape[2] == (1, self.hidden_size)\n",
    "        \n",
    "        x = h_n[-1, :, :]\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def hidden_state_from_rnn_outputs(self, rnn_outputs: Any) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Given the outputs from the forward pass through the torch \n",
    "        RNN/LSTM, and returns only the h_n (n'th hidden state) tensor. \n",
    "        Not implemented here, but must be implemented in subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def training_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"test\")\n",
    "    \n",
    "    def generalized_step(\n",
    "        self, batch: th.Tensor, batch_idx: int, label: str\n",
    "    ) -> th.Tensor:\n",
    "        x, y = batch\n",
    "        predicted = self(x).squeeze(1)\n",
    "        loss = F.mse_loss(predicted, y)\n",
    "        self.log(f\"{label}_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "class TextVanillaRNN(GeneralizedTextRNN):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding: nn.Embedding, \n",
    "        hidden_size: int = 128, \n",
    "        num_layers: int = 1,\n",
    "        lr: float = 1e-3, \n",
    "        dropout: float = 0.5\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            nn.RNN, embedding, hidden_size, num_layers, lr, dropout\n",
    "        )\n",
    "    \n",
    "    def hidden_state_from_rnn_outputs(self, rnn_outputs: Any) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Given the outputs from the forward pass through the torch \n",
    "        RNN, and returns only the h_n (n'th hidden state) tensor. \n",
    "        \"\"\"\n",
    "        return rnn_outputs[1]\n",
    "        \n",
    "class TextLSTM(GeneralizedTextRNN):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding: nn.Embedding, \n",
    "        hidden_size: int = 128, \n",
    "        num_layers: int = 1,\n",
    "        lr: float = 1e-3, \n",
    "        dropout: float = 0.5\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            nn.LSTM, embedding, hidden_size, num_layers, lr, dropout\n",
    "        )\n",
    "    \n",
    "    def hidden_state_from_rnn_outputs(self, rnn_outputs: Any) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Given the outputs from the forward pass through the torch \n",
    "        LSTM, and returns only the h_n (n'th hidden state) tensor. \n",
    "        \"\"\"\n",
    "        return rnn_outputs[1][0]\n",
    "\n",
    "class TextDistilbert(pl.LightningModule):\n",
    "    def __init__(self, hf_model: Callable, lr: float = 1e-3) -> None:\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.hf_model = hf_model\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x: List[th.Tensor]) -> th.Tensor:\n",
    "        assert len(x) == 2\n",
    "        seqs, attention_masks = x\n",
    "        \n",
    "        logits = self.hf_model(seqs, attention_masks).logits\n",
    "        return self.sigmoid(logits[:, 1] - logits[:, 0])\n",
    "    \n",
    "    def training_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch: th.Tensor, batch_idx: int) -> th.Tensor:\n",
    "        return self.generalized_step(batch, batch_idx, \"test\")\n",
    "    \n",
    "    def generalized_step(\n",
    "        self, batch: th.Tensor, batch_idx: int, label: str\n",
    "    ) -> th.Tensor:\n",
    "        x, y = batch\n",
    "        predicted = self(x)\n",
    "        loss = F.mse_loss(predicted, y)\n",
    "        self.log(f\"{label}_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), self.lr)\n",
    "        return optimizer\n",
    "\n",
    "def construct_model(model_config: Dict, embedding: Embedding):\n",
    "    if model_config[\"model_arch\"] in [\"VanillaRNN\", \"LSTM\"]:\n",
    "        rnn_cls = (\n",
    "            TextVanillaRNN if model_config[\"model_arch\"] == \"VanillaRNN\" \n",
    "            else TextLSTM\n",
    "        )\n",
    "        \n",
    "        params = list(inspect.signature(rnn_cls).parameters)\n",
    "        relevant_params = [p for p in params if p != \"embedding\"]\n",
    "        hparams = {k: v for k, v in model_config.items() if (k in relevant_params)}\n",
    "        return rnn_cls(embedding, **hparams)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebfa778-9bf1-4c12-8590-d1231bd4c6ad",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d28b8f44-3e7c-4d13-b191-572c13087151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_run(run_config: Dict) -> float:\n",
    "    \"\"\"\n",
    "    Returns the performance metric. In this case, it's val_loss\n",
    "    \"\"\"\n",
    "    rc = run_config\n",
    "    \n",
    "    df = get_raw_data(rc[\"data_config\"][\"name\"])\n",
    "    texts = {}\n",
    "    texts[\"train\"], texts[\"val\"], texts[\"test\"] = random_split(\n",
    "        df.text, nums_from_fractions(len(df.text), rc[\"data_config\"][\"fractions\"])\n",
    "    )\n",
    "    \n",
    "    voc = None\n",
    "    embedding = None\n",
    "    oov_rates = {}\n",
    "    dss = {}  # datasets\n",
    "    model = None\n",
    "    if rc[\"hf_model_name\"]:\n",
    "        hf_tokenizer = AutoTokenizer.from_pretrained(rc[\"hf_model_name\"])\n",
    "        for label in [\"train\", \"val\", \"test\"]:\n",
    "            # TODO: test count_oov_rate with hf_tokenizer\n",
    "            oov_rates[label] = -1\n",
    "            dss[label] = TextDistilbertDataset(\n",
    "                df.iloc[texts[label].indices], hf_tokenizer\n",
    "            )\n",
    "        hf_model = AutoModelForSequenceClassification.from_pretrained(rc[\"hf_model_name\"])\n",
    "        model = TextDistilbert(hf_model)\n",
    "    else:  # use selected config to get tokenizer, vocab, embedding, datasets\n",
    "        tokenizer = get_tokenizer(**rc[\"tokenizer_config\"])\n",
    "\n",
    "        if rc[\"glove_embedding_config\"]:\n",
    "            voc, embedding = glove_voc_and_embedding(\n",
    "                rc[\"embedding_dim\"], rc[\"glove_embedding_config\"]\n",
    "            )\n",
    "        else:\n",
    "            voc = build_vocab_from_texts(texts[\"train\"], tokenizer)\n",
    "            embedding = Embedding(len(voc), rc[\"embedding_dim\"], padding_idx=voc[PAD_TOKEN])\n",
    "\n",
    "        for label in [\"train\", \"val\", \"test\"]:\n",
    "            oov_rates[label] = count_oov_rate(\n",
    "                seqs_from_texts(texts[label], tokenizer, voc), \n",
    "                voc\n",
    "            )\n",
    "            dss[label] = TextDataset(df.iloc[texts[label].indices], tokenizer, voc)\n",
    "        \n",
    "        model = construct_model(rc[\"model_config\"], embedding)\n",
    "    \n",
    "    dls = {}  # dataloaders\n",
    "    for label in [\"train\", \"val\", \"test\"]:\n",
    "        shuffle = True if label == \"train\" else False\n",
    "        dls[label] = DataLoader(\n",
    "            dss[label], \n",
    "            batch_size=rc[\"data_config\"][\"batch_size\"], \n",
    "            shuffle=shuffle,\n",
    "            num_workers=rc[\"data_config\"][\"num_workers\"]\n",
    "        )\n",
    "\n",
    "    logger = WandbLogger(**rc[\"wandb_config\"])\n",
    "    logger.watch(model, log=\"all\")\n",
    "\n",
    "    # log more stuff\n",
    "    wandb.log(dict(\n",
    "        run_config = wandb.Table(\n",
    "            columns=list(rc.keys()),\n",
    "            data=[list(rc.values())],\n",
    "        ),\n",
    "    ))\n",
    "    wandb.log(dict(\n",
    "        oov_rate_train=oov_rates[\"train\"],\n",
    "        oov_rate_val=oov_rates[\"val\"],\n",
    "        oov_rate_test=oov_rates[\"test\"],\n",
    "        voc_size=len(voc) if voc else None,\n",
    "    ))\n",
    "\n",
    "    trainer = pl.Trainer(logger=logger, **rc[\"trainer_config\"])\n",
    "    trainer.fit(model, dls[\"train\"], dls[\"val\"])\n",
    "    wandb.finish()\n",
    "    \n",
    "    return trainer.logged_metrics['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8dec1eae-9625-4d8d-8dc4-d117d6438a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-19 13:58:00,388]\u001b[0m A new study created in memory with name: no-name-4ea436b8-748d-413b-8f6a-1d01cf35732c\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffc34f-9b4f-457a-b709-d88c1baec5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name     | Type                                | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | hf_model | DistilBertForSequenceClassification | 67.0 M\n",
      "1 | sigmoid  | Sigmoid                             | 0     \n",
      "-----------------------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennis/repos/sentimental/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b699618c3d9b4f76a466e8c307ab3361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennis/repos/sentimental/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 61216... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b00cd570ca54a2398438819c5630e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.53MB of 0.53MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_config = dict(\n",
    "    data_config = dict(\n",
    "        name=\"twitter_sentiment140_random_small\",\n",
    "        fractions=[0.7, 0.15, -1],\n",
    "        batch_size=64,\n",
    "        num_workers=0,  # default is 0\n",
    "    ),\n",
    "    hf_model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    tokenizer_config = None,\n",
    "    embedding_dim = None,\n",
    "    glove_embedding_config = None,  # contains keys: name, freeze_embedding\n",
    "    model_config = None,\n",
    "    wandb_config = dict(\n",
    "        project='scratch', \n",
    "        log_model=False\n",
    "    ),\n",
    "    trainer_config = dict(\n",
    "        max_epochs=10,\n",
    "        gpus=None,\n",
    "    )\n",
    ")\n",
    "\n",
    "val_loss = perform_run(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b720b-6a45-4d3d-93f4-9970bf930924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1):\n",
    "#     trial = study.ask()\n",
    "#     trial_hparams = dict(\n",
    "#         hidden_size=trial.suggest_int('hidden_size', 64, 256),\n",
    "#         dropout=trial.suggest_uniform('dropout', 0.1, 0.8),\n",
    "#         lr=trial.suggest_loguniform('lr', 1e-5, 1e-3),\n",
    "#         num_layers=trial.suggest_categorical('num_layers', [1,2,3,4,5,6])\n",
    "#     )\n",
    "    \n",
    "#     run_config = dict(\n",
    "#         data_config = dict(\n",
    "#             name=\"twitter_sentiment140_random_small\",\n",
    "#             fractions=[0.7, 0.15, -1],\n",
    "#             batch_size=64,\n",
    "#             num_workers=0,  # default is 0\n",
    "#         ),\n",
    "#         hf_model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "#         tokenizer_config = dict(\n",
    "#             tokenizer=\"spacy\",\n",
    "#             language=\"en_core_web_sm\"\n",
    "#         ),\n",
    "#         embedding_dim = 100,\n",
    "#         glove_embedding_config = None,  # contains keys: name, freeze_embedding\n",
    "#         model_config = dict(\n",
    "#             model_arch=\"LSTM\",\n",
    "#             num_layers=trial_hparams[\"num_layers\"],\n",
    "#             hidden_size=trial_hparams[\"hidden_size\"],\n",
    "#             lr=trial_hparams[\"lr\"],\n",
    "#             dropout=trial_hparams[\"dropout\"],\n",
    "#             layer_norm=False,\n",
    "#             residual_connections=False,\n",
    "#             loss_fn=\"MSELoss\",\n",
    "#         ),\n",
    "#         wandb_config = dict(\n",
    "#             project='expt2b_datasetSentiment140Small30k', \n",
    "#             log_model=False\n",
    "#         ),\n",
    "#         trainer_config = dict(\n",
    "#             max_epochs=40,\n",
    "#             gpus=None,\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     val_loss = perform_run(run_config)\n",
    "#     study.tell(trial, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526850-c70a-442d-9858-bbbb43fc374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "907d08d1-cdfc-4ae2-9934-fe40101b7e92",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b878cf-e873-48fa-80ab-857b6df40a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
